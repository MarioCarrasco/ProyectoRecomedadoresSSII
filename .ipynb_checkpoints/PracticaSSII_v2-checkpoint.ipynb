{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "synthetic-platform",
   "metadata": {},
   "source": [
    "# Con que páginas vamos a trabajar\n",
    "• El Mundo (https://elmundo.es) \n",
    "- Salud: https://www.elmundo.es/ciencia-y-salud/salud.html \n",
    "- Tecnología: https://www.elmundo.es/tecnologia.html \n",
    "- Ciencia: https://www.elmundo.es/ciencia-y-salud/ciencia.html \n",
    " \n",
    "• El País (https://elpais.com/) \n",
    "- Sanidad: https://elpais.com/noticias/sanidad/ \n",
    "- Tecnología: https://elpais.com/tecnologia/ \n",
    "- Ciencia: https://elpais.com/ciencia/ \n",
    "\n",
    "• 20 Minutos (https://www.20minutos.es/) \n",
    "- Salud: https://www.20minutos.es/salud/ \n",
    "- Tecnología: https://www.20minutos.es/tecnologia/ \n",
    "- Ciencia: https://www.20minutos.es/ciencia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-edinburgh",
   "metadata": {},
   "source": [
    "# Importamos las librerias que vamos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "played-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "# Librerías para calcular la similitud del coseno y el TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-outdoors",
   "metadata": {},
   "source": [
    "# 1.) Comenzamos accediendo a las URLs y sacando los links de las noticias de cada campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifty-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElMundoSalud = requests.get(\"https://www.elmundo.es/ciencia-y-salud/salud.html\")\n",
    "ElMundoTecnologia = requests.get(\"https://www.elmundo.es/tecnologia.html\")\n",
    "ElMundoCiencia = requests.get(\"https://www.elmundo.es/ciencia-y-salud/ciencia.html\")\n",
    "\n",
    "ElPaisSanidad = requests.get(\"https://elpais.com/noticias/sanidad/\")\n",
    "ElPaisTecnologia = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "ElPaisCiencia = requests.get(\"https://elpais.com/ciencia/\")\n",
    "\n",
    "MinutosSalud = requests.get(\"https://www.20minutos.es/salud/\")\n",
    "MinutosTecnologia = requests.get(\"https://www.20minutos.es/tecnologia/\")\n",
    "MinutosCiencia = requests.get(\"https://www.20minutos.es/ciencia/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expected-capability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# vamos a ver si ha accedido correctamente a todas las paginas (200)\n",
    "print(ElMundoSalud.status_code)\n",
    "print(ElMundoTecnologia.status_code)\n",
    "print(ElMundoCiencia.status_code)\n",
    "\n",
    "print(ElPaisSanidad.status_code)\n",
    "print(ElPaisTecnologia.status_code)\n",
    "print(ElPaisCiencia.status_code)\n",
    "\n",
    "print(MinutosSalud.status_code)\n",
    "print(MinutosTecnologia.status_code)\n",
    "print(MinutosCiencia.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alive-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descargamos el contenido de las páginas\n",
    "ContenidoElMundoSalud = BeautifulSoup (ElMundoSalud.content, 'html.parser')\n",
    "ContenidoElMundoTecnologia = BeautifulSoup (ElMundoTecnologia.content, 'html.parser')\n",
    "ContenidoElMundoCiencia = BeautifulSoup (ElMundoCiencia.content, 'html.parser')\n",
    "\n",
    "ContenidoElPaisSanidad = BeautifulSoup (ElPaisSanidad.content, 'html.parser')\n",
    "ContenidoElPaisTecnologia = BeautifulSoup (ElPaisTecnologia.content, 'html.parser')\n",
    "ContenidoElPaisCiencia = BeautifulSoup (ElPaisCiencia.content, 'html.parser')\n",
    "\n",
    "ContenidoMinutosSalud = BeautifulSoup (MinutosSalud.content, 'html.parser')\n",
    "ContenidoMinutosTecnologia = BeautifulSoup (MinutosTecnologia.content, 'html.parser')\n",
    "ContenidoMinutosCiencia = BeautifulSoup (MinutosCiencia.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-europe",
   "metadata": {},
   "source": [
    "## Noticias El Mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respiratory-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenLinksMundo(contenido, array):\n",
    "    a = contenido.find_all('a', class_=\"ue-c-cover-content__link\")\n",
    "    for link in a:\n",
    "        array.append(link[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-bones",
   "metadata": {},
   "source": [
    "#### Salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "buried-quality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksSalud = []\n",
    "obtenLinksMundo(ContenidoElMundoSalud, linksSalud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-cincinnati",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forty-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksTecnologia = []\n",
    "obtenLinksMundo(ContenidoElMundoTecnologia, linksTecnologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-lighting",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pointed-variable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksCiencia = []\n",
    "obtenLinksMundo(ContenidoElMundoCiencia, linksCiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-exemption",
   "metadata": {},
   "source": [
    "## Noticias El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "innocent-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenLinksPais(contenido, array):\n",
    "    h2 = contenido.find_all('h2', class_=\"c_t\")\n",
    "    for a in h2:\n",
    "        for link in a:\n",
    "            array.append(\"https://elpais.com/\"+link[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-corrections",
   "metadata": {},
   "source": [
    "#### Sanidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "otherwise-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksSanidadElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisSanidad, linksSanidadElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-interim",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "veterinary-citation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksTecnologiaElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisTecnologia, linksTecnologiaElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-milton",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stock-summit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksCienciaElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisCiencia, linksCienciaElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-potato",
   "metadata": {},
   "source": [
    "## Noticias 20 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adjustable-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo para recuperar todos los links de \"20 Minutos\"\n",
    "\n",
    "def obtenLinksMinutos(Contenido, array):\n",
    "    div = Contenido.find_all('div', class_='media-content')\n",
    "    for header in div:\n",
    "        for a in header.findAll(\"a\"):\n",
    "            array.append(a[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-leave",
   "metadata": {},
   "source": [
    "#### Salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "checked-welding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosSalud = []\n",
    "obtenLinksMinutos(ContenidoMinutosSalud, linksMinutosSalud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-protest",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "still-medium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosTecnologia = []\n",
    "obtenLinksMinutos(ContenidoMinutosTecnologia, linksMinutosTecnologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-homeless",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "inclusive-coaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosCiencia = []\n",
    "obtenLinksMinutos(ContenidoMinutosCiencia, linksMinutosCiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-acceptance",
   "metadata": {},
   "source": [
    "# 2.) Vamos a sacar el contenido que queremos de cada pagina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-mechanics",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"El Mundo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "duplicate-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosMundo(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"ElMundo/\"+area+\"/\"+str(area)+\".\"+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            archivo.write(str(titulo)+\"\\n\\n\"+str(subtitulo)+\"\\n\")\n",
    "\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    archivo.write(\"\\n\"+each.get_text())\n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksSalud, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksTecnologia, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksCiencia, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-attachment",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"El País\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosPais(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"ElPais/\"+area+\"/\"+str(area)+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            archivo.write(str(titulo)+\"\\n\\n\"+str(subtitulo)+\"\\n\")\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    try:\n",
    "                        archivo.write(\"\\n\"+each.get_text())\n",
    "                    except:\n",
    "                        archivo.write(\" \")\n",
    "\n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksSanidadElPáis, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksCienciaElPáis, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksTecnologiaElPáis, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-moderator",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"20 Minutos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosMinutos(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"20Minutos/\"+area+\"/\"+str(area)+\".\"+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            archivo.write(str(titulo)+\"\\n\\n\")\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        archivo.write(str(each.get_text()))\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            archivo.write(\"\\n\\n\")\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    archivo.write(cont.get_text())\n",
    "                except:\n",
    "                    archivo.write(\" \")\n",
    "            \n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosSalud, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosCiencia, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosTecnologia, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-pension",
   "metadata": {},
   "source": [
    "# Métodos para obtener las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-theme",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El Mundo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-marina",
   "metadata": {},
   "source": [
    "He decidido eliminar todos los espacios y caracteres especiales para que las coincidencias entre los diferentes medios sean más faciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "scenic-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contenidoEtiquetasElMundo(arraySalud, arrayCiencia, arrayTecnologia, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "#------------SALUD-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arraySalud[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "            #print(texto)\n",
    "                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                if (li!=[]):\n",
    "                    for each in li:\n",
    "                        etiquetas.append(each.get_text())\n",
    "                        for i in range(len(etiquetas)):\n",
    "                            etiquetas[i] = re.sub(\"\\ \",\"\",etiquetas[i])\n",
    "                            etiquetas[i] = etiquetas[i].lower()\n",
    "                else:\n",
    "                    etiquetas = \"\"\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------CIENCIA-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arrayCiencia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "            #print(texto)\n",
    "                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                if (li!=[]):\n",
    "                    for each in li:\n",
    "                        etiquetas.append(each.get_text())\n",
    "                        for i in range(len(etiquetas)):\n",
    "                            etiquetas[i] = re.sub(\"\\ \",\"\",etiquetas[i])\n",
    "                            etiquetas[i] = etiquetas[i].lower()\n",
    "                else:\n",
    "                    etiquetas = \"\"\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------TECNOLOGIA-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arrayTecnologia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                if (li!=[]):\n",
    "                    for each in li:\n",
    "                        etiquetas.append(each.get_text())\n",
    "                        for i in range(len(etiquetas)):\n",
    "                            etiquetas[i] = re.sub(\"\\ \",\"\",etiquetas[i])\n",
    "                            etiquetas[i] = etiquetas[i].lower()\n",
    "                else:\n",
    "                    etiquetas = \"\"\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "    df.drop(df.loc[(df['etiquetas']=='')].index, inplace=True)\n",
    "    #df.reset_index(inplace=True, drop=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-reunion",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "complex-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoEtiquetasElPais(arraySalud, arrayCiencia, arrayTecnologia, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "#------------SALUD-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arraySalud[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            import re\n",
    "            ul = soup.find('ul', class_=\"_df _ls\")\n",
    "            etiquetas = []\n",
    "            for each in ul:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "#------------CIENCIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayCiencia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            import re\n",
    "            ul = soup.find('ul', class_=\"_df _ls\")\n",
    "            etiquetas = []\n",
    "            for each in ul:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------TECNOLOGIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayTecnologia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            try:\n",
    "                import re\n",
    "                ul = soup.find('ul', class_=\"_df _ls\")\n",
    "                etiquetas = []\n",
    "                for each in ul:\n",
    "                    etiquetas.append(each.get_text())\n",
    "                    for i in range(len(etiquetas)):\n",
    "                        etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                        etiquetas[i] = etiquetas[i].lower()\n",
    "            except:\n",
    "                etiquetas = ''\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "    df.drop(df.loc[df['etiquetas']==''].index, inplace=True)\n",
    "    #df.reset_index(inplace=True, drop=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-flood",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "average-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoEtiquetasMinutos(arraySalud, arrayCiencia, arrayTecnologia, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "#------------SALUD-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arraySalud[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            texto = str(titulo)+\"\\n\\n\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        texto = str(texto)+str(each.get_text())\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            texto = texto+\"\\n\"\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    texto = texto+cont.get_text()\n",
    "                except:\n",
    "                    texto = str(texto)+\"\"\n",
    "            \n",
    "            #sacamos las etiquetas\n",
    "            li = soup.find_all('li', class_=\"tag\")\n",
    "            etiquetas = []\n",
    "            for each in li:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "                \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "        \n",
    "#------------CIENCIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayCiencia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            texto = str(titulo)+\"\\n\\n\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        texto = str(texto)+str(each.get_text())\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            texto = texto+\"\\n\"\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    texto = str(texto)+cont.get_text()\n",
    "                except:\n",
    "                    texto = str(texto)+\"\"\n",
    "            \n",
    "            #sacamos las etiquetas\n",
    "            li = soup.find_all('li', class_=\"tag\")\n",
    "            etiquetas = []\n",
    "            for each in li:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "                \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------TECNOLOGIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayTecnologia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            texto = str(titulo)+\"\\n\\n\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        texto = str(texto)+str(each.get_text())\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            texto = str(texto)+\"\\n\"\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    texto = str(texto)+cont.get_text()\n",
    "                except:\n",
    "                    texto = texto+\"\"\n",
    "            \n",
    "            #sacamos las etiquetas\n",
    "            li = soup.find_all('li', class_=\"tag\")\n",
    "            etiquetas = []\n",
    "            for each in li:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "                \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "    df.drop(df.loc[df['etiquetas']==''].index, inplace=True)\n",
    "    #df.reset_index(inplace=True, drop=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-pharmacy",
   "metadata": {},
   "source": [
    "# 3.) Tratamiento de las noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-bangkok",
   "metadata": {},
   "source": [
    "Este método guardamos en un dataframe \"df\" los diferentes ficheros de cada tematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "knowing-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia,ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia,ruta_PaisCiencia,ruta_PaisSalud,ruta_PaisTecnologia):\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSalud = os.listdir(ruta_PaisSalud)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    #importamos a un df las noticias\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tamil-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_20Mins(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia):\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "\n",
    "    #importamos a un df las noticias\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "\n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pleased-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_ElMundo(ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia):\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "\n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "administrative-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_ElPais(ruta_PaisCiencia,ruta_PaisSalud,ruta_PaisTecnologia):\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSalud = os.listdir(ruta_PaisSalud)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-instruction",
   "metadata": {},
   "source": [
    "Este metodo compara una noticia con el resto dentro del df y devuelve un dataframe con cada texto y la similitud que tiene con el texto introducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "continental-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compararNoticiasActualzado(textoin, df):\n",
    "    newdf = pd.DataFrame(columns = ['porcentage','data'])\n",
    "    \n",
    "    f = open(\"stopwords.txt\")\n",
    "    mis_stopwords = f.read() \n",
    "    vectorizer = TfidfVectorizer(stop_words=set(mis_stopwords))\n",
    "\n",
    "    #vectorizer = TfidfVectorizer()\n",
    "    for i in range(len(df)):\n",
    "        X = vectorizer.fit_transform([textoin,df.data[i]])\n",
    "        similarity_matrix = cosine_similarity(X,X)\n",
    "        porcentage = float(similarity_matrix[1][0])\n",
    "        #print(porcentaje)\n",
    "        texto = df.data[i]\n",
    "        new_row = {'porcentage':porcentage,'data': texto}\n",
    "        newdf = newdf.append(new_row, ignore_index=True)\n",
    "        i+=1\n",
    "    \n",
    "    newdf = newdf.sort_values('porcentage',ascending=False)\n",
    "    newdf = newdf.reset_index()\n",
    "    newdf = newdf.drop(['index'], axis=1)\n",
    "    newdf = newdf.drop([0],axis=0)\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-nickel",
   "metadata": {},
   "source": [
    "Este método recibe una lista de etiquetas y aplica sorensen dice para ver su similitud con el resto de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "transsexual-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicarSorensenDice(etiqueta_seleccionada, df):\n",
    "    df['similitud'] = '' #creamos la nueva columna con las similitudes\n",
    "    A = len(etiqueta_seleccionada)\n",
    "    a = etiqueta_seleccionada\n",
    "    for i in range(len(df.etiquetas)):\n",
    "        b = df.etiquetas[i]\n",
    "        B = len(df.etiquetas[i])\n",
    "        AyB = len(set(a).intersection(b))\n",
    "        sorensen=(2*AyB)/(A+B)\n",
    "        df.similitud[i] = sorensen\n",
    "    df = df.sort_values('similitud',ascending=False)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(['index'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-quebec",
   "metadata": {},
   "source": [
    "# 4.) Ventana (GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "rental-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLO PARA PRUEBAS\n",
    "df = pd.DataFrame(columns = ['data','etiquetas'])\n",
    "df = contenidoEtiquetasElMundo(linksSalud, linksCiencia, linksTecnologia, df)\n",
    "df = contenidoEtiquetasElPais(linksSanidadElPáis, linksCienciaElPáis, linksTecnologiaElPáis, df)\n",
    "df = contenidoEtiquetasMinutos(linksMinutosSalud, linksMinutosCiencia, linksMinutosTecnologia, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "focused-confidentiality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covid hoy, última hora en directo | España sum...</td>\n",
       "      <td>[cienciaysalud, coronavirus, covid19, variante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trasplantan riñones de cerdo editados genética...</td>\n",
       "      <td>[cienciaysalud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Una familia de Mérida denuncia que han vacunad...</td>\n",
       "      <td>[badajoz, vacunas, colegios, covid19, coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Síntomas de ómicron: Dolor muscular, fatiga y ...</td>\n",
       "      <td>[coronavirus, covid19, cienciaysalud, variante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qué quiere decir realmente convivir con un vir...</td>\n",
       "      <td>[hbpr, coronavirus, covid19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Alertan de falta de seguridad en la aplicación...</td>\n",
       "      <td>[juegosolímpicos, pekín, ciberseguridad, aplic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Desinstala esta app de tu iPhone: te está cobr...</td>\n",
       "      <td>[iphone, apple, ios, aplicacionesmóvil, redesy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Un error del navegador Safari filtra el histor...</td>\n",
       "      <td>[google, apple, seguridadinformática, internet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Rusia desmantela a REvil, la banda de cibercri...</td>\n",
       "      <td>[ciberseguridad, ransomware, ciberataque]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Cómo encontrar a alguien en WhatsApp si no apa...</td>\n",
       "      <td>[mensajeríainstantánea, chat, android, redesso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  data  \\\n",
       "0    Covid hoy, última hora en directo | España sum...   \n",
       "1    Trasplantan riñones de cerdo editados genética...   \n",
       "2    Una familia de Mérida denuncia que han vacunad...   \n",
       "3    Síntomas de ómicron: Dolor muscular, fatiga y ...   \n",
       "4    Qué quiere decir realmente convivir con un vir...   \n",
       "..                                                 ...   \n",
       "170  Alertan de falta de seguridad en la aplicación...   \n",
       "171  Desinstala esta app de tu iPhone: te está cobr...   \n",
       "172  Un error del navegador Safari filtra el histor...   \n",
       "173  Rusia desmantela a REvil, la banda de cibercri...   \n",
       "174  Cómo encontrar a alguien en WhatsApp si no apa...   \n",
       "\n",
       "                                             etiquetas  \n",
       "0    [cienciaysalud, coronavirus, covid19, variante...  \n",
       "1                                      [cienciaysalud]  \n",
       "2    [badajoz, vacunas, colegios, covid19, coronavi...  \n",
       "3    [coronavirus, covid19, cienciaysalud, variante...  \n",
       "4                         [hbpr, coronavirus, covid19]  \n",
       "..                                                 ...  \n",
       "170  [juegosolímpicos, pekín, ciberseguridad, aplic...  \n",
       "171  [iphone, apple, ios, aplicacionesmóvil, redesy...  \n",
       "172  [google, apple, seguridadinformática, internet...  \n",
       "173          [ciberseguridad, ransomware, ciberataque]  \n",
       "174  [mensajeríainstantánea, chat, android, redesso...  \n",
       "\n",
       "[175 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ready-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cienciaysalud']]\n",
      "[['iphone', 'realidadvirtual', 'appleinc.']]\n",
      "[['ciencia', 'universo', 'bigbang', 'estrellas', 'astrofísica']]\n",
      "[['ciencia', 'tonga', 'erupcionesvolcanes', 'volcanes', 'tsunami', 'polinesia', 'meteorología', 'aemet', 'desastresnaturales', 'españa']]\n",
      "[['gimnasio', 'nutrición', 'proteínas']]\n"
     ]
    }
   ],
   "source": [
    "# MAIN CON LA VENTANA\n",
    "sg.theme('BlueMono')   # Add a little color to your windows\n",
    "# All the stuff inside your window. This is the PSG magic code compactor...\n",
    "\n",
    "listaNumero = [\"1\", \"2\", \"3\", \"4\", \"5\"] #numero de noticias que se quieren en el ranking\n",
    "list1 = [\"20 minutos\", \"El Pais\", \"El Mundo\", \"Todos\"] #Filtrar las noticias por el medio de origen o seleccionar todos\n",
    "params = 10\n",
    "medios = [\"20 minutos\", \"El Pais\", \"El Mundo\"]\n",
    "categorias = [\"Ciencia\", \"Salud\", \"Tecnologia\"]\n",
    "noticias = [\"noticia 1\", \"noticia 2\", \"noticia 3\", \"noticia 4\", \"noticia 5\", \"noticia 6\", \"noticia 7\"]\n",
    "noticiasPorcentajes = [\"\"]\n",
    "\n",
    "\n",
    "busqueda = [[sg.Text('Consulta:'), sg.InputText(size=80, key= 'textbox_buscar_noticia')],\n",
    "            [sg.Text('TOP-N:'),sg.Combo(listaNumero, size = params, key= 'combo_topN_consulta'),sg.Text('             '), sg.Text('Filtrar:'), sg.Combo(list1, size = params, key = 'combo_filtrar'),sg.Text('                              '), sg.Button('Buscar', size=10, key='btn_buscarNoticias')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('Ranking:'), sg.Button('Preview', key='btn_preview_busqueda'), sg.Text('                                                      '), sg.Text('Texto de la noticia:')],\n",
    "            [sg.Listbox(values=[], size=(30, 10), key='listbox_ranking_noticias'),sg.Text('             ') ,sg.Multiline(size = (50, 10), key = 'textbox_preview_noticia')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "comparar = [[sg.Text('Medio:'), sg.Text('                                  '), sg.Text('Categoría:'), sg.Text('                                                             '), sg.Text('Noticias:')],\n",
    "            [sg.Listbox(medios, size=(20, 4), key='listbox_medios'),sg.Text('     '), sg.Listbox(categorias, size=(20, 4), key='listbox_categorias'),sg.Text('     '), sg.Button(\"Ver\\nnoticias\", size=(10,2), key='btn_verNoticias'),sg.Text('     '), sg.Listbox(values=[], size=(30, 4), key='listbox_noticias')],\n",
    "            [sg.Button('Preview', key='btn_preview')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview')],\n",
    "            [sg.Text('                                      '), sg.Text('TOP-N:'), sg.Combo(listaNumero, size = params, key='combo_topN'),sg.Text('             '), sg.Button('Buscar', size=10, key='btn_buscar')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('                        '), sg.Text('Ranking:'), sg.Listbox(values=\"\", size=(30, 5), key='noticias_porcentajes'), sg.Button('Preview', key='btn_preview_reslutado')],\n",
    "            [sg.Text('Texto de la noticia:')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview_reslutado')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "\n",
    "recomendar = [[sg.Text('Etiquetas:'), sg.Button('Preview', key='btn_preview_etiquetas_recomendar')],\n",
    "            [sg.Text('              '), sg.Listbox(values=\"\", size=(90, 7), key='listbox_etiquetas_recomendar')],\n",
    "            [sg.Text('                                      '), sg.Text('TOP-N:'), sg.Combo(listaNumero, size = params, key='combo_topN_recomendar'), sg.Text('             '), sg.Button('Buscar', size=10, key='btn_buscar_recomendar')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('                        '), sg.Text('Ranking:'), sg.Listbox(values=\"\", size=(50, 5), key='noticias_porcentajes_recomendar'), sg.Button('Preview', key='btn_preview_reslutado_recomendar')],\n",
    "            [sg.Text('Texto de la noticia:')],\n",
    "            [sg.Multiline(size = (107, 13), key = 'textbox_preview_reslutado_recomendar')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "\n",
    "tabgrp = [\n",
    "    [sg.TabGroup([[ sg.Tab('Comparador de noticias', comparar),\n",
    "                    sg.Tab('Buscador de noticias', busqueda),\n",
    "                    sg.Tab('Recomendador de noticias', recomendar)\n",
    "                ]], tab_location='top')]]\n",
    "\n",
    "# Create the Window\n",
    "window = sg.Window('Buscador y comparador de noticias', tabgrp)\n",
    "# Event Loop to process \"events\"\n",
    "while True:             \n",
    "    event, values = window.read()\n",
    "    if event in (sg.WIN_CLOSED, 'Salir'):\n",
    "        break\n",
    "\n",
    "    if event == \"btn_verNoticias\":\n",
    "        eleccion = values['listbox_medios']+values['listbox_categorias']\n",
    "        print(eleccion)\n",
    "        if(eleccion==['20 minutos', 'Ciencia']):\n",
    "            noticias = os.listdir(\"20Minutos/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['20 minutos', 'Salud']):\n",
    "            noticias = os.listdir(\"20Minutos/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['20 minutos', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"20Minutos/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "        elif(eleccion==['El Pais', 'Ciencia']):\n",
    "            noticias = os.listdir(\"ElPais/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Pais', 'Salud']):\n",
    "            noticias = os.listdir(\"ElPais/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Pais', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"ElPais/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "        elif(eleccion==['El Mundo', 'Ciencia']):\n",
    "            noticias = os.listdir(\"ElMundo/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Mundo', 'Salud']):\n",
    "            noticias = os.listdir(\"ElMundo/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Mundo', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"ElMundo/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "    if event==\"btn_preview\":\n",
    "        noticia = values['listbox_noticias']\n",
    "        f = open(eleccion[0].replace(\" \", \"\")+\"/\"+eleccion[1]+\"/\"+noticia[0], \"r\")\n",
    "        texto = f.read()\n",
    "        window['textbox_preview'].update(texto)\n",
    "        \n",
    "    if event==\"btn_buscar\":\n",
    "        eleccion = values['listbox_medios']+values['listbox_categorias']\n",
    "        noticia = values['listbox_noticias']\n",
    "        topN=values['combo_topN']\n",
    "        dirNoticia = open(eleccion[0].replace(\" \", \"\")+\"/\"+eleccion[1]+\"/\"+noticia[0], \"r\")\n",
    "        textoNoticia = dirNoticia.read()\n",
    "        df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "        dataRanking = compararNoticiasActualzado(textoNoticia,df)\n",
    "        rankingLimitado = []\n",
    "        textoRanking = []\n",
    "        for i in range(int(topN)):\n",
    "            i+=1\n",
    "            # REVISAR\n",
    "            rankingLimitado.append(str(int(dataRanking.porcentage[i]*100))+\" % de coincidencia\")\n",
    "            textoRanking.append(dataRanking.data[i])   \n",
    "\n",
    "        window.find_element('noticias_porcentajes').Update(values=rankingLimitado)\n",
    "        \n",
    "    if event==\"btn_preview_reslutado\":\n",
    "        textoResultado = \"\"        \n",
    "        for i in range(len(rankingLimitado)):\n",
    "            if str(values['noticias_porcentajes'][0])==str(rankingLimitado[i]):   \n",
    "                textoResultado = textoRanking[i]\n",
    "            i+=1\n",
    "        window['textbox_preview_reslutado'].update(textoResultado)\n",
    "    \n",
    "        #window['textbox_preview_noticia'].update(textoNoticia)\n",
    "        #window['textbox_preview_noticia'].update(textoNoticia)\n",
    "    \n",
    "    if event==\"btn_preview_busqueda\":\n",
    "        textoResultado = \"\"        \n",
    "        for i in range(len(rankingLimitado2)):\n",
    "            if str(values['listbox_ranking_noticias'][0])==str(rankingLimitado2[i]):   \n",
    "                textoResultado = textoRanking2[i]\n",
    "            i+=1\n",
    "        window['textbox_preview_noticia'].update(textoResultado)\n",
    "        \n",
    "    if event == \"btn_buscarNoticias\":\n",
    "        consulta = values['textbox_buscar_noticia']\n",
    "        topN=values['combo_topN_consulta']\n",
    "        filtro = values['combo_filtrar']\n",
    "        print(filtro)\n",
    "        with open('consulta.txt', 'w') as f:\n",
    "            f.write(consulta)\n",
    "        dirNoticia2 = open(\"consulta.txt\", \"r\")\n",
    "        textoConsulta = dirNoticia2.read()\n",
    "        #df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "        if(filtro=='20 minutos'):\n",
    "            df = cargar_noticias_20Mins(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='El Pais'):\n",
    "            df = cargar_noticias_ElPais(\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='El Mundo'):\n",
    "            df = cargar_noticias_ElMundo(\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='Todos'):\n",
    "            df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "            \n",
    "        else:\n",
    "            df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        #dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "        #rankingLimitado2 = []\n",
    "        #textoRanking2 = []\n",
    "        #for i in range(int(topN)):\n",
    "         #   i+=1\n",
    "            #REVISAR\n",
    "          #  rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "           # textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "        #window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "    if event == \"btn_preview_etiquetas_recomendar\":\n",
    "        #df = pd.DataFrame(columns = ['data','etiquetas'])\n",
    "        #df = contenidoEtiquetasElMundo(linksSalud, linksCiencia, linksTecnologia, df)\n",
    "        #df = contenidoEtiquetasElPais(linksSanidadElPáis, linksCienciaElPáis, linksTecnologiaElPáis, df)\n",
    "        #df = contenidoEtiquetasMinutos(linksMinutosSalud, linksMinutosCiencia, linksMinutosTecnologia, df)\n",
    "        window.find_element('listbox_etiquetas_recomendar').Update(values=df.etiquetas)\n",
    "    \n",
    "    if event == \"btn_buscar_recomendar\":\n",
    "        etiqueta_seleccionada = values[\"listbox_etiquetas_recomendar\"]\n",
    "        topN = values[\"combo_topN_recomendar\"]\n",
    "        print(etiqueta_seleccionada)\n",
    "        # Ahora llamamos al método que aplicará sorensen dice entre las etiquetas seleccionadas y las del dataframe\n",
    "        newdf = aplicarSorensenDice(etiqueta_seleccionada[0], df)\n",
    "        resultados_etiquetas2 = []\n",
    "        ranking_texto2 = []\n",
    "        for i in range(int(topN[0])):\n",
    "            resultados_etiquetas2.append(\"Similitud de etiquetas: \"+ str(newdf.similitud[i]))\n",
    "            ranking_texto2.append(str(newdf.data[i]))\n",
    "        window.find_element('noticias_porcentajes_recomendar').Update(values=resultados_etiquetas2)\n",
    "        \n",
    "    if event == \"btn_preview_reslutado_recomendar\":\n",
    "        texto_Resultado = \"\"        \n",
    "        for i in range(len(ranking_texto2)):\n",
    "            if str(values['noticias_porcentajes_recomendar'][0])==str(resultados_etiquetas2[i]):   \n",
    "                textoResultado = ranking_texto2[i]\n",
    "            i+=1\n",
    "        window['textbox_preview_reslutado_recomendar'].update(textoResultado)\n",
    "\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-thursday",
   "metadata": {},
   "source": [
    "# PRUEBA.) Tratamiento de las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes de un documento con el resto (el  resto estan en un dataframe)\n",
    "def compararNoticias(texto, df):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similitudes = []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            X = vectorizer.fit_transform([texto,df.data[i]])\n",
    "            similarity_matrix = cosine_similarity(X,X)\n",
    "            similitudes.append(float(similarity_matrix[1][0]))\n",
    "            i+=1\n",
    "        except:\n",
    "            i+=1\n",
    "    similitudes.sort(reverse=True) # Los ordemanos de mayor a menor \n",
    "    similitudes.remove(similitudes[0]) # Eliminamos el primer valor porque nos da la similitud con el propio elemento, por loo que siempre va a ser 1 (no nos sirve de nada)\n",
    "    return similitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-raise",
   "metadata": {},
   "source": [
    "Este método guardamos en un dataframe \"df\" los diferentes ficheros de cada tematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia,ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia,ruta_PaisCiencia,ruta_PaisSanidad,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSanidad = os.listdir(ruta_PaisSanidad)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "\n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSanidad:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSanidad+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-generic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar un documento (obtenemos los vectores de los documentos que queramos)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer ()\n",
    "\n",
    "X = vectorizer.fit_transform([df.data[1],df.data[130]])\n",
    "vectorizer.get_feature_names()\n",
    "vectorDoc = X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF. Similitud entre dos documentos\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer ()\n",
    "X = vectorizer.fit_transform([df.data[1],df.data[250]])\n",
    "similarity_matrix = cosine_similarity(X,X)\n",
    "similarity_matrix[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes de un documento con el resto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similitudes = []\n",
    "for i in range(len(df)):\n",
    "    vectorizer = TfidfVectorizer ()\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer ()\n",
    "        X = vectorizer.fit_transform([df.data[250],df.data[i]])\n",
    "        similarity_matrix = cosine_similarity(X,X)\n",
    "        similitudes.append(float(similarity_matrix[1][0]))\n",
    "        i+=1\n",
    "    except:\n",
    "        i+=1\n",
    "similitudes.sort(reverse=True) # Los ordemanos de mayor a menor \n",
    "similitudes.remove(similitudes[0]) # Eliminamos el primer valor porque nos da la similitud con el propio elemento, por loo que siempre va a ser 1 (no nos sirve de nada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar un documento y calcular su similitud con todos los documentos\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i in range(len(df)):\n",
    "    vectorizer = TfidfVectorizer ()\n",
    "    try:\n",
    "        X = vectorizer.fit_transform([df.data[1],df.data[i]])\n",
    "        vectorDoc = X.toarray()\n",
    "        print(vectorDoc)\n",
    "        i+=1\n",
    "    except:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una lista con los símbolos de puntuación y las stopwords para que sean retiradas del texto\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lista_parada = open(os.getcwd()+\"/stopwords.txt\",\"r\").read().split()\n",
    "non_words = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para el preprocesado del texto\n",
    "def remove_stop_words(dirty_text):\n",
    "    cleaned_text = ''\n",
    "    for word in dirty_text.split():\n",
    "        if word in language_stopwords or word in non_words:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_text += word + ' '\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation(dirty_string):\n",
    "    for word in non_words:\n",
    "        dirty_string = dirty_string.replace(word, '')\n",
    "    return dirty_string\n",
    "\n",
    "def process_file(file_name):\n",
    "    file_content = open(file_name, \"r\").read()\n",
    "    # All to lower case\n",
    "    file_content = file_content.lower()\n",
    "    # Remove punctuation and spanish stopwords\n",
    "    file_content = remove_punctuation(file_content)\n",
    "    file_content = remove_stop_words(file_content)\n",
    "    return file_content\n",
    "\n",
    "def procesado(ruta):\n",
    "    for file in (os.listdir(ruta)):\n",
    "        process_file(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesado(\"20Minutos/Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el preprocesado del texto\n",
    "import os\n",
    "\n",
    "os.listdir(\"20Minutos/Salud\")\n",
    "os.listdir(\"20Minutos/Ciencia\")\n",
    "os.listdir(\"20Minutos/Tecnologia\")\n",
    "os.listdir(\"ElMundo/Ciencia\")\n",
    "os.listdir(\"ElMundo/Salud\")\n",
    "os.listdir(\"ElMundo/Tecnologia\")\n",
    "os.listdir(\"ElPais/Ciencia\")\n",
    "os.listdir(\"ElPais/Sanidad\")\n",
    "os.listdir(\"ElPais/Tecnologia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-helmet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
