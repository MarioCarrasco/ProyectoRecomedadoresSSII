{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "organizational-pursuit",
   "metadata": {},
   "source": [
    "# Con que páginas vamos a trabajar\n",
    "• El Mundo (https://elmundo.es) \n",
    "- Salud: https://www.elmundo.es/ciencia-y-salud/salud.html \n",
    "- Tecnología: https://www.elmundo.es/tecnologia.html \n",
    "- Ciencia: https://www.elmundo.es/ciencia-y-salud/ciencia.html \n",
    " \n",
    "• El País (https://elpais.com/) \n",
    "- Sanidad: https://elpais.com/noticias/sanidad/ \n",
    "- Tecnología: https://elpais.com/tecnologia/ \n",
    "- Ciencia: https://elpais.com/ciencia/ \n",
    "\n",
    "• 20 Minutos (https://www.20minutos.es/) \n",
    "- Salud: https://www.20minutos.es/salud/ \n",
    "- Tecnología: https://www.20minutos.es/tecnologia/ \n",
    "- Ciencia: https://www.20minutos.es/ciencia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-decimal",
   "metadata": {},
   "source": [
    "# Importamos las librerias que vamos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "limiting-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-delaware",
   "metadata": {},
   "source": [
    "# 1.) Comenzamos accediendo a las URLs y sacando los links de las noticias de cada campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "centered-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElMundoSalud = requests.get(\"https://www.elmundo.es/ciencia-y-salud/salud.html\")\n",
    "ElMundoTecnologia = requests.get(\"https://www.elmundo.es/tecnologia.html\")\n",
    "ElMundoCiencia = requests.get(\"https://www.elmundo.es/ciencia-y-salud/ciencia.html\")\n",
    "\n",
    "ElPaisSanidad = requests.get(\"https://elpais.com/noticias/sanidad/\")\n",
    "ElPaisTecnologia = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "ElPaisCiencia = requests.get(\"https://elpais.com/ciencia/\")\n",
    "\n",
    "MinutosSalud = requests.get(\"https://www.20minutos.es/salud/\")\n",
    "MinutosTecnologia = requests.get(\"https://www.20minutos.es/tecnologia/\")\n",
    "MinutosCiencia = requests.get(\"https://www.20minutos.es/ciencia/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "younger-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# vamos a ver si ha accedido correctamente a todas las paginas (200)\n",
    "print(ElMundoSalud.status_code)\n",
    "print(ElMundoTecnologia.status_code)\n",
    "print(ElMundoCiencia.status_code)\n",
    "\n",
    "print(ElPaisSanidad.status_code)\n",
    "print(ElPaisTecnologia.status_code)\n",
    "print(ElPaisCiencia.status_code)\n",
    "\n",
    "print(MinutosSalud.status_code)\n",
    "print(MinutosTecnologia.status_code)\n",
    "print(MinutosCiencia.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "false-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descargamos el contenido de las páginas\n",
    "ContenidoElMundoSalud = BeautifulSoup (ElMundoSalud.content, 'html.parser')\n",
    "ContenidoElMundoTecnologia = BeautifulSoup (ElMundoTecnologia.content, 'html.parser')\n",
    "ContenidoElMundoCiencia = BeautifulSoup (ElMundoCiencia.content, 'html.parser')\n",
    "\n",
    "ContenidoElPaisSanidad = BeautifulSoup (ElPaisSanidad.content, 'html.parser')\n",
    "ContenidoElPaisTecnologia = BeautifulSoup (ElPaisTecnologia.content, 'html.parser')\n",
    "ContenidoElPaisCiencia = BeautifulSoup (ElPaisCiencia.content, 'html.parser')\n",
    "\n",
    "ContenidoMinutosSalud = BeautifulSoup (MinutosSalud.content, 'html.parser')\n",
    "ContenidoMinutosTecnologia = BeautifulSoup (MinutosTecnologia.content, 'html.parser')\n",
    "ContenidoMinutosCiencia = BeautifulSoup (MinutosCiencia.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-surrey",
   "metadata": {},
   "source": [
    "## Noticias El Mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "short-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenLinksMundo(contenido, array):\n",
    "    a = contenido.find_all('a', class_=\"ue-c-cover-content__link\")\n",
    "    for link in a:\n",
    "        array.append(link[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-coalition",
   "metadata": {},
   "source": [
    "#### Salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civic-underground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksSalud = []\n",
    "obtenLinksMundo(ContenidoElMundoSalud, linksSalud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-effect",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "expressed-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksTecnologia = []\n",
    "obtenLinksMundo(ContenidoElMundoTecnologia, linksTecnologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-operations",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "celtic-camcorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksCiencia = []\n",
    "obtenLinksMundo(ContenidoElMundoCiencia, linksCiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-affair",
   "metadata": {},
   "source": [
    "## Noticias El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caroline-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenLinksPais(contenido, array):\n",
    "    h2 = contenido.find_all('h2', class_=\"c_t\")\n",
    "    for a in h2:\n",
    "        for link in a:\n",
    "            array.append(\"https://elpais.com/\"+link[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-fourth",
   "metadata": {},
   "source": [
    "#### Sanidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "velvet-lecture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksSanidadElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisSanidad, linksSanidadElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-thermal",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blind-blowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksTecnologiaElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisTecnologia, linksTecnologiaElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-reminder",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cultural-stroke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksCienciaElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisCiencia, linksCienciaElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-phase",
   "metadata": {},
   "source": [
    "### Noticias 20 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "preceding-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo para recuperar todos los links de \"20 Minutos\"\n",
    "\n",
    "def obtenLinksMinutos(Contenido, array):\n",
    "    div = Contenido.find_all('div', class_='media-content')\n",
    "    for header in div:\n",
    "        for a in header.findAll(\"a\"):\n",
    "            array.append(a[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-glossary",
   "metadata": {},
   "source": [
    "#### Salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "professional-ottawa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosSalud = []\n",
    "obtenLinksMinutos(ContenidoMinutosSalud, linksMinutosSalud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-geography",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "burning-bermuda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosTecnologia = []\n",
    "obtenLinksMinutos(ContenidoMinutosTecnologia, linksMinutosTecnologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-contract",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "altered-regard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosCiencia = []\n",
    "obtenLinksMinutos(ContenidoMinutosCiencia, linksMinutosCiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-medium",
   "metadata": {},
   "source": [
    "# 2.) Vamos a sacar el contenido que queremos de cada pagina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-masters",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"El Mundo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosMundo(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"ElMundo/\"+area+\"/\"+str(area)+\".\"+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            archivo.write(str(titulo)+\"\\n\\n\"+str(subtitulo)+\"\\n\")\n",
    "\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    archivo.write(\"\\n\"+each.get_text())\n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksSalud, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksTecnologia, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksCiencia, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-register",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"El País\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosPais(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"ElPais/\"+area+\"/\"+str(area)+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            archivo.write(str(titulo)+\"\\n\\n\"+str(subtitulo)+\"\\n\")\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    try:\n",
    "                        archivo.write(\"\\n\"+each.get_text())\n",
    "                    except:\n",
    "                        archivo.write(\" \")\n",
    "\n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksSanidadElPáis, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksCienciaElPáis, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksTecnologiaElPáis, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-monster",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"20 Minutos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosMinutos(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"20Minutos/\"+area+\"/\"+str(area)+\".\"+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            archivo.write(str(titulo)+\"\\n\\n\")\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        archivo.write(str(each.get_text()))\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            archivo.write(\"\\n\\n\")\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    archivo.write(cont.get_text())\n",
    "                except:\n",
    "                    archivo.write(\" \")\n",
    "            \n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosSalud, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosCiencia, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosTecnologia, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-spectrum",
   "metadata": {},
   "source": [
    "## Método para obtener las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-speech",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El Mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "artificial-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contenidoEtiquetasElMundo(arraySalud, arrayCiencia, arrayTecnologia, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "#------------SALUD-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arraySalud[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "            #print(texto)\n",
    "                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                for each in li:\n",
    "                    etiquetas.append(each.get_text())\n",
    "                etiquetas_columna = \" \".join(etiquetas)\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas_columna}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------CIENCIA-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arrayCiencia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "            #print(texto)\n",
    "                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                for each in li:\n",
    "                    etiquetas.append(each.get_text())\n",
    "                etiquetas_columna = \" \".join(etiquetas)\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas_columna}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------TECNOLOGIA-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arrayTecnologia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                for each in li:\n",
    "                    etiquetas.append(each.get_text())\n",
    "                etiquetas_columna = \" \".join(etiquetas)\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas_columna}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "    df.drop(df.loc[df['etiquetas']==''].index, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "increased-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = ['data','etiquetas'])\n",
    "df = contenidoEtiquetasElMundo(linksSalud, linksCiencia, linksTecnologia, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-baking",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "hidden-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoEtiquetasElPais(arraySalud, arrayCiencia, arrayTecnologia, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "#------------SALUD-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arraySalud[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            import re\n",
    "            ul = soup.findAll('ul', class_=\"_df _ls\")\n",
    "            etiquetas_columna = \"\"\n",
    "            for each in ul:\n",
    "                etiquetas_columna = each.get_text()\n",
    "            etiquetas_columna = re.sub(r\"\\B([A-Z])\", r\" \\1\", etiquetas_columna)\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas_columna}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "#------------CIENCIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayCiencia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            import re\n",
    "            ul = soup.findAll('ul', class_=\"_df _ls\")\n",
    "            etiquetas_columna = \"\"\n",
    "            for each in ul:\n",
    "                etiquetas_columna = each.get_text()\n",
    "            etiquetas_columna = re.sub(r\"\\B([A-Z])\", r\" \\1\", etiquetas_columna)\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas_columna}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------TECNOLOGIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayTecnologia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            import re\n",
    "            ul = soup.findAll('ul', class_=\"_df _ls\")\n",
    "            etiquetas_columna = \"\"\n",
    "            for each in ul:\n",
    "                etiquetas_columna = each.get_text()\n",
    "            etiquetas_columna = re.sub(r\"\\B([A-Z])\", r\" \\1\", etiquetas_columna)\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas_columna}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "    df.drop(df.loc[df['etiquetas']==''].index, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "behavioral-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = ['data','etiquetas'])\n",
    "df = contenidoEtiquetasElPais(linksSanidadElPáis, linksCienciaElPáis, linksTecnologiaElPáis, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-asthma",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoEtiquetasMinutos(array, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            texto = titulo+\"\\n\\n\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        texto = texto+str(each.get_text())\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            texto = texto+\"\\n\"\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    texto = texto+cont.get_text()\n",
    "                except:\n",
    "                    texto = texto+\"\"\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas_columna}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = ['data','etiquetas'])\n",
    "df = contenidoEtiquetasMinutos(linksMinutosSalud, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-event",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "distant-assistant",
   "metadata": {},
   "source": [
    "# 3.) Tratamiento de las noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-observer",
   "metadata": {},
   "source": [
    "Este método guardamos en un dataframe \"df\" los diferentes ficheros de cada tematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia,ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia,ruta_PaisCiencia,ruta_PaisSalud,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSalud = os.listdir(ruta_PaisSalud)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_20Mins(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "\n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "\n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_ElMundo(ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia):\n",
    "    import os\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "\n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_ElPais(ruta_PaisCiencia,ruta_PaisSalud,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSalud = os.listdir(ruta_PaisSalud)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-aluminum",
   "metadata": {},
   "source": [
    "Este metodo compara una noticia con el resto dentro del df y devuelve un dataframe con cada texto y la similitud que tiene con el texto introducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONA\n",
    "def compararNoticiasActualzado(textoin, df):\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    newdf = pd.DataFrame(columns = ['porcentage','data'])\n",
    "    \n",
    "    f = open(\"stopwords.txt\")\n",
    "    mis_stopwords = f.read() \n",
    "    vectorizer = TfidfVectorizer(stop_words=set(mis_stopwords))\n",
    "\n",
    "    #vectorizer = TfidfVectorizer()\n",
    "    for i in range(len(df)):\n",
    "        X = vectorizer.fit_transform([textoin,df.data[i]])\n",
    "        similarity_matrix = cosine_similarity(X,X)\n",
    "        porcentage = float(similarity_matrix[1][0])\n",
    "        #print(porcentaje)\n",
    "        texto = df.data[i]\n",
    "        new_row = {'porcentage':porcentage,'data': texto}\n",
    "        newdf = newdf.append(new_row, ignore_index=True)\n",
    "        i+=1\n",
    "    \n",
    "    newdf = newdf.sort_values('porcentage',ascending=False)\n",
    "    newdf = newdf.reset_index()\n",
    "    newdf = newdf.drop(['index'], axis=1)\n",
    "    newdf = newdf.drop([0],axis=0)\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-syndrome",
   "metadata": {},
   "source": [
    "# 4.) Ventana (GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CON LA VENTANA\n",
    "\n",
    "import os\n",
    "import PySimpleGUI as sg\n",
    "sg.theme('BlueMono')   # Add a little color to your windows\n",
    "# All the stuff inside your window. This is the PSG magic code compactor...\n",
    "\n",
    "listaNumero = [\"1\", \"2\", \"3\", \"4\", \"5\"] #numero de noticias que se quieren en el ranking\n",
    "list1 = [\"20 minutos\", \"El Pais\", \"El Mundo\", \"Todos\"] #Filtrar las noticias por el medio de origen o seleccionar todos\n",
    "params = 10\n",
    "medios = [\"20 minutos\", \"El Pais\", \"El Mundo\"]\n",
    "categorias = [\"Ciencia\", \"Salud\", \"Tecnologia\"]\n",
    "noticias = [\"noticia 1\", \"noticia 2\", \"noticia 3\", \"noticia 4\", \"noticia 5\", \"noticia 6\", \"noticia 7\"]\n",
    "noticiasPorcentajes = [\"\"]\n",
    "\n",
    "\n",
    "busqueda = [[sg.Text('Consulta:'), sg.InputText(size=80, key= 'textbox_buscar_noticia')],\n",
    "            [sg.Text('TOP-N:'),sg.Combo(listaNumero, size = params, key= 'combo_topN_consulta'),sg.Text('             '), sg.Text('Filtrar:'), sg.Combo(list1, size = params, key = 'combo_filtrar'),sg.Text('                              '), sg.Button('Buscar', size=10, key='btn_buscarNoticias')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('Ranking:'), sg.Button('Preview', key='btn_preview_busqueda'), sg.Text('                                                      '), sg.Text('Texto de la noticia:')],\n",
    "            [sg.Listbox(values=[], size=(30, 10), key='listbox_ranking_noticias'),sg.Text('             ') ,sg.Multiline(size = (50, 10), key = 'textbox_preview_noticia')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "comparar = [[sg.Text('Medio:'), sg.Text('                                  '), sg.Text('Categoría:'), sg.Text('                                                             '), sg.Text('Noticias:')],\n",
    "            [sg.Listbox(medios, size=(20, 4), key='listbox_medios'),sg.Text('     '), sg.Listbox(categorias, size=(20, 4), key='listbox_categorias'),sg.Text('     '), sg.Button(\"Ver\\nnoticias\", size=(10,2), key='btn_verNoticias'),sg.Text('     '), sg.Listbox(values=[], size=(30, 4), key='listbox_noticias')],\n",
    "            [sg.Button('Preview', key='btn_preview')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview')],\n",
    "            [sg.Text('                                      '), sg.Text('TOP-N:'), sg.Combo(listaNumero, size = params, key='combo_topN'),sg.Text('             '), sg.Button('Buscar', size=10, key='btn_buscar')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('                        '), sg.Text('Ranking:'), sg.Listbox(values=\"\", size=(30, 5), key='noticias_porcentajes'), sg.Button('Preview', key='btn_preview_reslutado')],\n",
    "            [sg.Text('Texto de la noticia:')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview_reslutado')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "\n",
    "recomendar = [[sg.Text('Medio:'), sg.Text('                                  '), sg.Text('Categoría:'), sg.Text('                                                             '), sg.Text('Noticias:')],\n",
    "            [sg.Listbox(medios, size=(20, 4), key='listbox_medios_recomendar'),sg.Text('     '), sg.Listbox(categorias, size=(20, 4), key='listbox_categorias_recomendar'),sg.Text('     '), sg.Button(\"Ver\\nnoticias\", size=(10,2), key='btn_verNoticias_recomendar'),sg.Text('     '), sg.Listbox(values=[], size=(30, 4), key='listbox_noticias_recomendar')],\n",
    "            [sg.Button('Preview', key='btn_preview_recomendar')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview_recomendar')],\n",
    "            [sg.Text('                                      '), sg.Text('TOP-N:'), sg.Combo(listaNumero, size = params, key='combo_topN_recomendar'),sg.Text('             '), sg.Button('Buscar', size=10, key='btn_buscar_recomendar')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('                        '), sg.Text('Filtrar:'), sg.Combo(medios, size = params, key=\"combo_filtro_recomendar\"),sg.Text('Ranking:'), sg.Listbox(values=\"\", size=(30, 5), key='noticias_porcentajes_recomendar'), sg.Button('Preview', key='btn_preview_reslutado_recomendar')],\n",
    "            [sg.Text('Texto de la noticia:')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview_reslutado_recomendar')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "\n",
    "tabgrp = [\n",
    "    [sg.TabGroup([[ sg.Tab('Comparador de noticias', comparar),\n",
    "                    sg.Tab('Buscador de noticias', busqueda),\n",
    "                    sg.Tab('Recomendador de noticias', recomendar)\n",
    "                ]], tab_location='top')]]\n",
    "\n",
    "# Create the Window\n",
    "window = sg.Window('Buscador y comparador de noticias', tabgrp)\n",
    "# Event Loop to process \"events\"\n",
    "while True:             \n",
    "    event, values = window.read()\n",
    "    if event in (sg.WIN_CLOSED, 'Salir'):\n",
    "        break\n",
    "\n",
    "    if event == \"btn_verNoticias\":\n",
    "        eleccion = values['listbox_medios']+values['listbox_categorias']\n",
    "        print(eleccion)\n",
    "        if(eleccion==['20 minutos', 'Ciencia']):\n",
    "            noticias = os.listdir(\"20Minutos/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['20 minutos', 'Salud']):\n",
    "            noticias = os.listdir(\"20Minutos/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['20 minutos', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"20Minutos/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "        elif(eleccion==['El Pais', 'Ciencia']):\n",
    "            noticias = os.listdir(\"ElPais/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Pais', 'Salud']):\n",
    "            noticias = os.listdir(\"ElPais/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Pais', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"ElPais/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "        elif(eleccion==['El Mundo', 'Ciencia']):\n",
    "            noticias = os.listdir(\"ElMundo/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Mundo', 'Salud']):\n",
    "            noticias = os.listdir(\"ElMundo/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Mundo', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"ElMundo/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "    if event==\"btn_preview\":\n",
    "        noticia = values['listbox_noticias']\n",
    "        f = open(eleccion[0].replace(\" \", \"\")+\"/\"+eleccion[1]+\"/\"+noticia[0], \"r\")\n",
    "        texto = f.read()\n",
    "        window['textbox_preview'].update(texto)\n",
    "        \n",
    "    if event==\"btn_buscar\":\n",
    "        eleccion = values['listbox_medios']+values['listbox_categorias']\n",
    "        noticia = values['listbox_noticias']\n",
    "        topN=values['combo_topN']\n",
    "        dirNoticia = open(eleccion[0].replace(\" \", \"\")+\"/\"+eleccion[1]+\"/\"+noticia[0], \"r\")\n",
    "        textoNoticia = dirNoticia.read()\n",
    "        df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "        dataRanking = compararNoticiasActualzado(textoNoticia,df)\n",
    "        rankingLimitado = []\n",
    "        textoRanking = []\n",
    "        for i in range(int(topN)):\n",
    "            i+=1\n",
    "            # REVISAR\n",
    "            rankingLimitado.append(str(int(dataRanking.porcentage[i]*100))+\" % de coincidencia\")\n",
    "            textoRanking.append(dataRanking.data[i])   \n",
    "\n",
    "        window.find_element('noticias_porcentajes').Update(values=rankingLimitado)\n",
    "        \n",
    "    if event==\"btn_preview_reslutado\":\n",
    "        textoResultado = \"\"        \n",
    "        for i in range(len(rankingLimitado)):\n",
    "            if str(values['noticias_porcentajes'][0])==str(rankingLimitado[i]):   \n",
    "                textoResultado = textoRanking[i]\n",
    "            i+=1\n",
    "        window['textbox_preview_reslutado'].update(textoResultado)\n",
    "    \n",
    "        #window['textbox_preview_noticia'].update(textoNoticia)\n",
    "        #window['textbox_preview_noticia'].update(textoNoticia)\n",
    "    \n",
    "    if event==\"btn_preview_busqueda\":\n",
    "        textoResultado = \"\"        \n",
    "        for i in range(len(rankingLimitado2)):\n",
    "            if str(values['listbox_ranking_noticias'][0])==str(rankingLimitado2[i]):   \n",
    "                textoResultado = textoRanking2[i]\n",
    "            i+=1\n",
    "        window['textbox_preview_noticia'].update(textoResultado)\n",
    "        \n",
    "    if event == \"btn_buscarNoticias\":\n",
    "        consulta = values['textbox_buscar_noticia']\n",
    "        topN=values['combo_topN_consulta']\n",
    "        filtro = values['combo_filtrar']\n",
    "        print(filtro)\n",
    "        with open('consulta.txt', 'w') as f:\n",
    "            f.write(consulta)\n",
    "        dirNoticia2 = open(\"consulta.txt\", \"r\")\n",
    "        textoConsulta = dirNoticia2.read()\n",
    "        #df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "        if(filtro=='20 minutos'):\n",
    "            df = cargar_noticias_20Mins(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='El Pais'):\n",
    "            df = cargar_noticias_ElPais(\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='El Mundo'):\n",
    "            df = cargar_noticias_ElMundo(\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='Todos'):\n",
    "            df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "            \n",
    "        else:\n",
    "            df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        #dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "        #rankingLimitado2 = []\n",
    "        #textoRanking2 = []\n",
    "        #for i in range(int(topN)):\n",
    "         #   i+=1\n",
    "            #REVISAR\n",
    "          #  rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "           # textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "        #window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PySimpleGUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-updating",
   "metadata": {},
   "source": [
    "# PRUEBA.) Tratamiento de las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes de un documento con el resto (el  resto estan en un dataframe)\n",
    "def compararNoticias(texto, df):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similitudes = []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            X = vectorizer.fit_transform([texto,df.data[i]])\n",
    "            similarity_matrix = cosine_similarity(X,X)\n",
    "            similitudes.append(float(similarity_matrix[1][0]))\n",
    "            i+=1\n",
    "        except:\n",
    "            i+=1\n",
    "    similitudes.sort(reverse=True) # Los ordemanos de mayor a menor \n",
    "    similitudes.remove(similitudes[0]) # Eliminamos el primer valor porque nos da la similitud con el propio elemento, por loo que siempre va a ser 1 (no nos sirve de nada)\n",
    "    return similitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-light",
   "metadata": {},
   "source": [
    "Este método guardamos en un dataframe \"df\" los diferentes ficheros de cada tematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia,ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia,ruta_PaisCiencia,ruta_PaisSanidad,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSanidad = os.listdir(ruta_PaisSanidad)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "\n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSanidad:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSanidad+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-battery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar un documento (obtenemos los vectores de los documentos que queramos)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer ()\n",
    "\n",
    "X = vectorizer.fit_transform([df.data[1],df.data[130]])\n",
    "vectorizer.get_feature_names()\n",
    "vectorDoc = X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF. Similitud entre dos documentos\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer ()\n",
    "X = vectorizer.fit_transform([df.data[1],df.data[250]])\n",
    "similarity_matrix = cosine_similarity(X,X)\n",
    "similarity_matrix[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes de un documento con el resto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similitudes = []\n",
    "for i in range(len(df)):\n",
    "    vectorizer = TfidfVectorizer ()\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer ()\n",
    "        X = vectorizer.fit_transform([df.data[250],df.data[i]])\n",
    "        similarity_matrix = cosine_similarity(X,X)\n",
    "        similitudes.append(float(similarity_matrix[1][0]))\n",
    "        i+=1\n",
    "    except:\n",
    "        i+=1\n",
    "similitudes.sort(reverse=True) # Los ordemanos de mayor a menor \n",
    "similitudes.remove(similitudes[0]) # Eliminamos el primer valor porque nos da la similitud con el propio elemento, por loo que siempre va a ser 1 (no nos sirve de nada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar un documento y calcular su similitud con todos los documentos\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i in range(len(df)):\n",
    "    vectorizer = TfidfVectorizer ()\n",
    "    try:\n",
    "        X = vectorizer.fit_transform([df.data[1],df.data[i]])\n",
    "        vectorDoc = X.toarray()\n",
    "        print(vectorDoc)\n",
    "        i+=1\n",
    "    except:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una lista con los símbolos de puntuación y las stopwords para que sean retiradas del texto\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lista_parada = open(os.getcwd()+\"/stopwords.txt\",\"r\").read().split()\n",
    "non_words = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para el preprocesado del texto\n",
    "def remove_stop_words(dirty_text):\n",
    "    cleaned_text = ''\n",
    "    for word in dirty_text.split():\n",
    "        if word in language_stopwords or word in non_words:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_text += word + ' '\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation(dirty_string):\n",
    "    for word in non_words:\n",
    "        dirty_string = dirty_string.replace(word, '')\n",
    "    return dirty_string\n",
    "\n",
    "def process_file(file_name):\n",
    "    file_content = open(file_name, \"r\").read()\n",
    "    # All to lower case\n",
    "    file_content = file_content.lower()\n",
    "    # Remove punctuation and spanish stopwords\n",
    "    file_content = remove_punctuation(file_content)\n",
    "    file_content = remove_stop_words(file_content)\n",
    "    return file_content\n",
    "\n",
    "def procesado(ruta):\n",
    "    for file in (os.listdir(ruta)):\n",
    "        process_file(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesado(\"20Minutos/Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el preprocesado del texto\n",
    "import os\n",
    "\n",
    "os.listdir(\"20Minutos/Salud\")\n",
    "os.listdir(\"20Minutos/Ciencia\")\n",
    "os.listdir(\"20Minutos/Tecnologia\")\n",
    "os.listdir(\"ElMundo/Ciencia\")\n",
    "os.listdir(\"ElMundo/Salud\")\n",
    "os.listdir(\"ElMundo/Tecnologia\")\n",
    "os.listdir(\"ElPais/Ciencia\")\n",
    "os.listdir(\"ElPais/Sanidad\")\n",
    "os.listdir(\"ElPais/Tecnologia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-dover",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
