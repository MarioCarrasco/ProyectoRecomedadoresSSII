{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "miniature-theorem",
   "metadata": {},
   "source": [
    "# Con que páginas vamos a trabajar\n",
    "• El Mundo (https://elmundo.es) \n",
    "- Salud: https://www.elmundo.es/ciencia-y-salud/salud.html \n",
    "- Tecnología: https://www.elmundo.es/tecnologia.html \n",
    "- Ciencia: https://www.elmundo.es/ciencia-y-salud/ciencia.html \n",
    " \n",
    "• El País (https://elpais.com/) \n",
    "- Sanidad: https://elpais.com/noticias/sanidad/ \n",
    "- Tecnología: https://elpais.com/tecnologia/ \n",
    "- Ciencia: https://elpais.com/ciencia/ \n",
    "\n",
    "• 20 Minutos (https://www.20minutos.es/) \n",
    "- Salud: https://www.20minutos.es/salud/ \n",
    "- Tecnología: https://www.20minutos.es/tecnologia/ \n",
    "- Ciencia: https://www.20minutos.es/ciencia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-arrest",
   "metadata": {},
   "source": [
    "# Importamos las librerias que vamos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "solved-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-stamp",
   "metadata": {},
   "source": [
    "# 1.) Comenzamos accediendo a las URLs y sacando los links de las noticias de cada campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collaborative-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElMundoSalud = requests.get(\"https://www.elmundo.es/ciencia-y-salud/salud.html\")\n",
    "ElMundoTecnologia = requests.get(\"https://www.elmundo.es/tecnologia.html\")\n",
    "ElMundoCiencia = requests.get(\"https://www.elmundo.es/ciencia-y-salud/ciencia.html\")\n",
    "\n",
    "ElPaisSanidad = requests.get(\"https://elpais.com/noticias/sanidad/\")\n",
    "ElPaisTecnologia = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "ElPaisCiencia = requests.get(\"https://elpais.com/ciencia/\")\n",
    "\n",
    "MinutosSalud = requests.get(\"https://www.20minutos.es/salud/\")\n",
    "MinutosTecnologia = requests.get(\"https://www.20minutos.es/tecnologia/\")\n",
    "MinutosCiencia = requests.get(\"https://www.20minutos.es/ciencia/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# vamos a ver si ha accedido correctamente a todas las paginas (200)\n",
    "print(ElMundoSalud.status_code)\n",
    "print(ElMundoTecnologia.status_code)\n",
    "print(ElMundoCiencia.status_code)\n",
    "\n",
    "print(ElPaisSanidad.status_code)\n",
    "print(ElPaisTecnologia.status_code)\n",
    "print(ElPaisCiencia.status_code)\n",
    "\n",
    "print(MinutosSalud.status_code)\n",
    "print(MinutosTecnologia.status_code)\n",
    "print(MinutosCiencia.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stunning-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descargamos el contenido de las páginas\n",
    "ContenidoElMundoSalud = BeautifulSoup (ElMundoSalud.content, 'html.parser')\n",
    "ContenidoElMundoTecnologia = BeautifulSoup (ElMundoTecnologia.content, 'html.parser')\n",
    "ContenidoElMundoCiencia = BeautifulSoup (ElMundoCiencia.content, 'html.parser')\n",
    "\n",
    "ContenidoElPaisSanidad = BeautifulSoup (ElPaisSanidad.content, 'html.parser')\n",
    "ContenidoElPaisTecnologia = BeautifulSoup (ElPaisTecnologia.content, 'html.parser')\n",
    "ContenidoElPaisCiencia = BeautifulSoup (ElPaisCiencia.content, 'html.parser')\n",
    "\n",
    "ContenidoMinutosSalud = BeautifulSoup (MinutosSalud.content, 'html.parser')\n",
    "ContenidoMinutosTecnologia = BeautifulSoup (MinutosTecnologia.content, 'html.parser')\n",
    "ContenidoMinutosCiencia = BeautifulSoup (MinutosCiencia.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-snowboard",
   "metadata": {},
   "source": [
    "## Noticias El Mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "valued-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenLinksMundo(contenido, array):\n",
    "    a = contenido.find_all('a', class_=\"ue-c-cover-content__link\")\n",
    "    for link in a:\n",
    "        array.append(link[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-wedding",
   "metadata": {},
   "source": [
    "#### Salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indie-tiffany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksSalud = []\n",
    "obtenLinksMundo(ContenidoElMundoSalud, linksSalud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-import",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mathematical-engineer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksTecnologia = []\n",
    "obtenLinksMundo(ContenidoElMundoTecnologia, linksTecnologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-friendship",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "concrete-coaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksCiencia = []\n",
    "obtenLinksMundo(ContenidoElMundoCiencia, linksCiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-vampire",
   "metadata": {},
   "source": [
    "## Noticias El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collaborative-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenLinksPais(contenido, array):\n",
    "    h2 = contenido.find_all('h2', class_=\"c_t\")\n",
    "    for a in h2:\n",
    "        for link in a:\n",
    "            array.append(\"https://elpais.com/\"+link[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-sauce",
   "metadata": {},
   "source": [
    "#### Sanidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "young-warning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksSanidadElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisSanidad, linksSanidadElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-yield",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alleged-authentication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksTecnologiaElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisTecnologia, linksTecnologiaElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-coordinator",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "difficult-yesterday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksCienciaElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisCiencia, linksCienciaElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-niger",
   "metadata": {},
   "source": [
    "### Noticias 20 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extraordinary-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo para recuperar todos los links de \"20 Minutos\"\n",
    "\n",
    "def obtenLinksMinutos(Contenido, array):\n",
    "    div = Contenido.find_all('div', class_='media-content')\n",
    "    for header in div:\n",
    "        for a in header.findAll(\"a\"):\n",
    "            array.append(a[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-butterfly",
   "metadata": {},
   "source": [
    "#### Salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "raised-special",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosSalud = []\n",
    "obtenLinksMinutos(ContenidoMinutosSalud, linksMinutosSalud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-commerce",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "official-sender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosTecnologia = []\n",
    "obtenLinksMinutos(ContenidoMinutosTecnologia, linksMinutosTecnologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-cradle",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "judicial-caribbean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksMinutosCiencia = []\n",
    "obtenLinksMinutos(ContenidoMinutosCiencia, linksMinutosCiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-plant",
   "metadata": {},
   "source": [
    "# 2.) Vamos a sacar el contenido que queremos de cada pagina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-polish",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"El Mundo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosMundo(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"ElMundo/\"+area+\"/\"+str(area)+\".\"+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            archivo.write(str(titulo)+\"\\n\\n\"+str(subtitulo)+\"\\n\")\n",
    "\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    archivo.write(\"\\n\"+each.get_text())\n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksSalud, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksTecnologia, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksCiencia, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-bruce",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"El País\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosPais(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"ElPais/\"+area+\"/\"+str(area)+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            archivo.write(str(titulo)+\"\\n\\n\"+str(subtitulo)+\"\\n\")\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    try:\n",
    "                        archivo.write(\"\\n\"+each.get_text())\n",
    "                    except:\n",
    "                        archivo.write(\" \")\n",
    "\n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksSanidadElPáis, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksCienciaElPáis, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksTecnologiaElPáis, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-potato",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"20 Minutos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosMinutos(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"20Minutos/\"+area+\"/\"+str(area)+\".\"+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            archivo.write(str(titulo)+\"\\n\\n\")\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        archivo.write(str(each.get_text()))\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            archivo.write(\"\\n\\n\")\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    archivo.write(cont.get_text())\n",
    "                except:\n",
    "                    archivo.write(\" \")\n",
    "            \n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosSalud, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosCiencia, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosTecnologia, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-alignment",
   "metadata": {},
   "source": [
    "## Métodos para obtener las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-marketplace",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El Mundo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-production",
   "metadata": {},
   "source": [
    "He decidido eliminar todos los espacios y caracteres especiales para que las coincidencias entre los diferentes medios sean más faciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "tight-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contenidoEtiquetasElMundo(arraySalud, arrayCiencia, arrayTecnologia, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "#------------SALUD-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arraySalud[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "            #print(texto)\n",
    "                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                if (li!=[]):\n",
    "                    for each in li:\n",
    "                        etiquetas.append(each.get_text())\n",
    "                        for i in range(len(etiquetas)):\n",
    "                            etiquetas[i] = re.sub(\"\\ \",\"\",etiquetas[i])\n",
    "                            etiquetas[i] = etiquetas[i].lower()\n",
    "                else:\n",
    "                    etiquetas = \"\"\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------CIENCIA-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arrayCiencia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "            #print(texto)\n",
    "                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                if (li!=[]):\n",
    "                    for each in li:\n",
    "                        etiquetas.append(each.get_text())\n",
    "                        for i in range(len(etiquetas)):\n",
    "                            etiquetas[i] = re.sub(\"\\ \",\"\",etiquetas[i])\n",
    "                            etiquetas[i] = etiquetas[i].lower()\n",
    "                else:\n",
    "                    etiquetas = \"\"\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------TECNOLOGIA-------------\n",
    "    for i in range(22):\n",
    "        req = requests.get(arrayTecnologia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())                    \n",
    "            #try catch para guardar las etiquetas y los textos en el dataframe\n",
    "            try:\n",
    "                li = soup.findAll('li', class_=\"ue-c-article__tags-item\")\n",
    "                etiquetas = []\n",
    "                if (li!=[]):\n",
    "                    for each in li:\n",
    "                        etiquetas.append(each.get_text())\n",
    "                        for i in range(len(etiquetas)):\n",
    "                            etiquetas[i] = re.sub(\"\\ \",\"\",etiquetas[i])\n",
    "                            etiquetas[i] = etiquetas[i].lower()\n",
    "                else:\n",
    "                    etiquetas = \"\"\n",
    "            except:\n",
    "                etiquetas_columna = \"\"\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "    df.drop(df.loc[(df['etiquetas']=='')].index, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "norwegian-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = ['data','etiquetas'])\n",
    "df = contenidoEtiquetasElMundo(linksSalud, linksCiencia, linksTecnologia, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "wicked-change",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covid hoy, última hora en directo | La primera...</td>\n",
       "      <td>[coronavirus, covid19, cienciaysalud, variante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sanidad comunica 94.472 contagios y la inciden...</td>\n",
       "      <td>[coronavirus, covid19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dos tercios de las reacciones adversas a la va...</td>\n",
       "      <td>[cienciaysalud, covid19, coronavirus, vacunas,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Muere de Covid la cantante checa Hana Horka, q...</td>\n",
       "      <td>[coronavirus, repúblicacheca, vacunas, covid19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Test de antígenos con muestras de saliva, ¿est...</td>\n",
       "      <td>[cienciaysalud, coronavirus, covid19, variante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Moderna proyecta una vacuna anual conjunta de ...</td>\n",
       "      <td>[vacunas, coronavirus, covid19, gripe, ciencia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>¿Qué tratamientos son efectivos contra ómicron...</td>\n",
       "      <td>[coronavirus, varianteómicron, covid19, cienci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18 meses muriendo de más: el exceso de falleci...</td>\n",
       "      <td>[cienciaysalud, covid19, coronavirus, vacunas,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Síntomas de ómicron: Dolor muscular, fatiga y ...</td>\n",
       "      <td>[coronavirus, covid19, cienciaysalud, variante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cuándo deben ponerse la tercera dosis las pers...</td>\n",
       "      <td>[coronavirus, covid19, varianteómicron]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Investigan en Hong Kong el contagio de Covid p...</td>\n",
       "      <td>[coronavirus, covid19, cienciaysalud, hongkong]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>La incidencia sube hasta 3.397 en un fin de se...</td>\n",
       "      <td>[coronavirus, covid19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Todo lo que debe saber sobre los cólicos nefrí...</td>\n",
       "      <td>[cienciaysalud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Un estudio apunta que la hidroxicloroquina ret...</td>\n",
       "      <td>[esclerosismúltiple, cienciaysalud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>El drama de una familia no vacunada destruida ...</td>\n",
       "      <td>[coronavirus, hbpr, covid19, italia, cienciays...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>La sexta ola empieza a perder fuerza\\n\\nLos co...</td>\n",
       "      <td>[cienciaysalud, covid19, coronavirus, variante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Un nuevo fósil de mamífero aragonés resuelve u...</td>\n",
       "      <td>[ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Luna llena de enero: cuándo llega y cómo ver l...</td>\n",
       "      <td>[cienciaysalud, ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>¿Qué ha dicho realmente la NASA sobre el aster...</td>\n",
       "      <td>[cienciaysalud, ciencia, hbpr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Una serie de \"mareas inusitadas\" por el volcán...</td>\n",
       "      <td>[ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>La erupción del volcán de Tonga, una de las má...</td>\n",
       "      <td>[ciencia, cienciaysalud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Un cúmulo estelar queda huérfano tras expulsar...</td>\n",
       "      <td>[artículosrafaelbachiller]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>La amenaza de un tsunami \"ya ha pasado\" tras l...</td>\n",
       "      <td>[cienciaysalud, ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Un tsunami provocado por una erupción en Tonga...</td>\n",
       "      <td>[australia, ciencia, cienciaysalud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Un meteorito cae en Sierra Morena, Ciudad Real...</td>\n",
       "      <td>[ciencia, ciudadreal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Un búho nival del Ártico, como el de Harry Pot...</td>\n",
       "      <td>[estadosunidos, canadá, suiza]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Identifican la segunda candidata a exoluna en ...</td>\n",
       "      <td>[ciencia, cienciaysalud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Encuentran en la Antártida la zona de cría de ...</td>\n",
       "      <td>[ciencia, alemania]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>'Sapiens' modernos ya vivían en la actual Etio...</td>\n",
       "      <td>[ciencia, cienciaysalud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Encuentran por casualidad un gigantesco 'dragó...</td>\n",
       "      <td>[ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Concluye el despliegue del telescopio espacial...</td>\n",
       "      <td>[cienciaysalud, ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Entra en erupción un volcán en Galápagos, dond...</td>\n",
       "      <td>[ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>El Pevolca reduce la zona de exclusión y este ...</td>\n",
       "      <td>[volcánlapalma, lapalma, ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Un nuevo método de extracción de muestras de A...</td>\n",
       "      <td>[ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>El plan de Sanidad para la tercera dosis en me...</td>\n",
       "      <td>[enfermedadesinfecciosas, vacunas, coronavirus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Microsoft alerta de un sofisticado ciberataque...</td>\n",
       "      <td>[microsoft, ucrania, rusia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Fortnite vuelve al iPhone y al iPad, aunque no...</td>\n",
       "      <td>[iphone, android, fortnite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Sony fabricará más Playstation 4 para aliviar ...</td>\n",
       "      <td>[sony, ps5, playstation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Probamos el Surface Laptop Studio: una idea lo...</td>\n",
       "      <td>[microsoft]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Honor se apunta a los plegables con el Magic V...</td>\n",
       "      <td>[huawei, samsung, xiaomi, google, europa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Kobo Sage: la vida (más allá del Kindle) puede...</td>\n",
       "      <td>[amazon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>El casco de realidad virtual de Apple podría r...</td>\n",
       "      <td>[iphone, realidadvirtual, appleinc.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Un error en Safari filtra el historial y los d...</td>\n",
       "      <td>[appleinc.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Rusia desmantela el conocido grupo de piratas ...</td>\n",
       "      <td>[rusia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Así serán las teles de 2022, según el CES\\n\\nP...</td>\n",
       "      <td>[samsung]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>La PS5 tendrá un casco de realidad virtual\\n\\n...</td>\n",
       "      <td>[playstation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>El CES arranca en Las Vegas bajo la sombra de ...</td>\n",
       "      <td>[varianteómicron, sony, samsung]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>La pandemia resucita al PC sobremesa\\n\\nLa ven...</td>\n",
       "      <td>[coronavirus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>WhatsApp: el triple check azul quiere proteger...</td>\n",
       "      <td>[whatsapp, android, telegram, markzuckerberg, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>El primer SMS de la historia se subastará en F...</td>\n",
       "      <td>[francia]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 data  \\\n",
       "0   Covid hoy, última hora en directo | La primera...   \n",
       "1   Sanidad comunica 94.472 contagios y la inciden...   \n",
       "2   Dos tercios de las reacciones adversas a la va...   \n",
       "3   Muere de Covid la cantante checa Hana Horka, q...   \n",
       "5   Test de antígenos con muestras de saliva, ¿est...   \n",
       "6   Moderna proyecta una vacuna anual conjunta de ...   \n",
       "7   ¿Qué tratamientos son efectivos contra ómicron...   \n",
       "8   18 meses muriendo de más: el exceso de falleci...   \n",
       "9   Síntomas de ómicron: Dolor muscular, fatiga y ...   \n",
       "10  Cuándo deben ponerse la tercera dosis las pers...   \n",
       "11  Investigan en Hong Kong el contagio de Covid p...   \n",
       "12  La incidencia sube hasta 3.397 en un fin de se...   \n",
       "13  Todo lo que debe saber sobre los cólicos nefrí...   \n",
       "14  Un estudio apunta que la hidroxicloroquina ret...   \n",
       "16  El drama de una familia no vacunada destruida ...   \n",
       "17  La sexta ola empieza a perder fuerza\\n\\nLos co...   \n",
       "22  Un nuevo fósil de mamífero aragonés resuelve u...   \n",
       "23  Luna llena de enero: cuándo llega y cómo ver l...   \n",
       "24  ¿Qué ha dicho realmente la NASA sobre el aster...   \n",
       "25  Una serie de \"mareas inusitadas\" por el volcán...   \n",
       "26  La erupción del volcán de Tonga, una de las má...   \n",
       "27  Un cúmulo estelar queda huérfano tras expulsar...   \n",
       "28  La amenaza de un tsunami \"ya ha pasado\" tras l...   \n",
       "29  Un tsunami provocado por una erupción en Tonga...   \n",
       "30  Un meteorito cae en Sierra Morena, Ciudad Real...   \n",
       "31  Un búho nival del Ártico, como el de Harry Pot...   \n",
       "32  Identifican la segunda candidata a exoluna en ...   \n",
       "33  Encuentran en la Antártida la zona de cría de ...   \n",
       "34  'Sapiens' modernos ya vivían en la actual Etio...   \n",
       "36  Encuentran por casualidad un gigantesco 'dragó...   \n",
       "37  Concluye el despliegue del telescopio espacial...   \n",
       "38  Entra en erupción un volcán en Galápagos, dond...   \n",
       "39  El Pevolca reduce la zona de exclusión y este ...   \n",
       "40  Un nuevo método de extracción de muestras de A...   \n",
       "42  El plan de Sanidad para la tercera dosis en me...   \n",
       "44  Microsoft alerta de un sofisticado ciberataque...   \n",
       "46  Fortnite vuelve al iPhone y al iPad, aunque no...   \n",
       "47  Sony fabricará más Playstation 4 para aliviar ...   \n",
       "48  Probamos el Surface Laptop Studio: una idea lo...   \n",
       "49  Honor se apunta a los plegables con el Magic V...   \n",
       "50  Kobo Sage: la vida (más allá del Kindle) puede...   \n",
       "52  El casco de realidad virtual de Apple podría r...   \n",
       "53  Un error en Safari filtra el historial y los d...   \n",
       "54  Rusia desmantela el conocido grupo de piratas ...   \n",
       "55  Así serán las teles de 2022, según el CES\\n\\nP...   \n",
       "57  La PS5 tendrá un casco de realidad virtual\\n\\n...   \n",
       "58  El CES arranca en Las Vegas bajo la sombra de ...   \n",
       "60  La pandemia resucita al PC sobremesa\\n\\nLa ven...   \n",
       "61  WhatsApp: el triple check azul quiere proteger...   \n",
       "65  El primer SMS de la historia se subastará en F...   \n",
       "\n",
       "                                            etiquetas  \n",
       "0   [coronavirus, covid19, cienciaysalud, variante...  \n",
       "1                              [coronavirus, covid19]  \n",
       "2   [cienciaysalud, covid19, coronavirus, vacunas,...  \n",
       "3     [coronavirus, repúblicacheca, vacunas, covid19]  \n",
       "5   [cienciaysalud, coronavirus, covid19, variante...  \n",
       "6   [vacunas, coronavirus, covid19, gripe, ciencia...  \n",
       "7   [coronavirus, varianteómicron, covid19, cienci...  \n",
       "8   [cienciaysalud, covid19, coronavirus, vacunas,...  \n",
       "9   [coronavirus, covid19, cienciaysalud, variante...  \n",
       "10            [coronavirus, covid19, varianteómicron]  \n",
       "11    [coronavirus, covid19, cienciaysalud, hongkong]  \n",
       "12                             [coronavirus, covid19]  \n",
       "13                                    [cienciaysalud]  \n",
       "14                [esclerosismúltiple, cienciaysalud]  \n",
       "16  [coronavirus, hbpr, covid19, italia, cienciays...  \n",
       "17  [cienciaysalud, covid19, coronavirus, variante...  \n",
       "22                                          [ciencia]  \n",
       "23                           [cienciaysalud, ciencia]  \n",
       "24                     [cienciaysalud, ciencia, hbpr]  \n",
       "25                                          [ciencia]  \n",
       "26                           [ciencia, cienciaysalud]  \n",
       "27                         [artículosrafaelbachiller]  \n",
       "28                           [cienciaysalud, ciencia]  \n",
       "29                [australia, ciencia, cienciaysalud]  \n",
       "30                              [ciencia, ciudadreal]  \n",
       "31                     [estadosunidos, canadá, suiza]  \n",
       "32                           [ciencia, cienciaysalud]  \n",
       "33                                [ciencia, alemania]  \n",
       "34                           [ciencia, cienciaysalud]  \n",
       "36                                          [ciencia]  \n",
       "37                           [cienciaysalud, ciencia]  \n",
       "38                                          [ciencia]  \n",
       "39                  [volcánlapalma, lapalma, ciencia]  \n",
       "40                                          [ciencia]  \n",
       "42  [enfermedadesinfecciosas, vacunas, coronavirus...  \n",
       "44                        [microsoft, ucrania, rusia]  \n",
       "46                        [iphone, android, fortnite]  \n",
       "47                           [sony, ps5, playstation]  \n",
       "48                                        [microsoft]  \n",
       "49          [huawei, samsung, xiaomi, google, europa]  \n",
       "50                                           [amazon]  \n",
       "52               [iphone, realidadvirtual, appleinc.]  \n",
       "53                                        [appleinc.]  \n",
       "54                                            [rusia]  \n",
       "55                                          [samsung]  \n",
       "57                                      [playstation]  \n",
       "58                   [varianteómicron, sony, samsung]  \n",
       "60                                      [coronavirus]  \n",
       "61  [whatsapp, android, telegram, markzuckerberg, ...  \n",
       "65                                          [francia]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-failure",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "charitable-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoEtiquetasElPais(arraySalud, arrayCiencia, arrayTecnologia, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "#------------SALUD-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arraySalud[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            import re\n",
    "            ul = soup.find('ul', class_=\"_df _ls\")\n",
    "            etiquetas = []\n",
    "            for each in ul:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "#------------CIENCIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayCiencia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            import re\n",
    "            ul = soup.find('ul', class_=\"_df _ls\")\n",
    "            etiquetas = []\n",
    "            for each in ul:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------TECNOLOGIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayTecnologia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            texto = titulo+\"\\n\\n\"+subtitulo+\"\\n\"\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    texto = texto+\"\\n\"+str(each.get_text())\n",
    "                    \n",
    "            # Etiqetas de la noticia\n",
    "            try:\n",
    "                import re\n",
    "                ul = soup.find('ul', class_=\"_df _ls\")\n",
    "                etiquetas = []\n",
    "                for each in ul:\n",
    "                    etiquetas.append(each.get_text())\n",
    "                    for i in range(len(etiquetas)):\n",
    "                        etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                        etiquetas[i] = etiquetas[i].lower()\n",
    "            except:\n",
    "                etiquetas = ''\n",
    "            \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "    df.drop(df.loc[df['etiquetas']==''].index, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fatty-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = ['data','etiquetas'])\n",
    "df = contenidoEtiquetasElPais(linksSanidadElPáis, linksCienciaElPáis, linksTecnologiaElPáis, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "collected-organ",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La trama del falso oro verde: desmantelada una...</td>\n",
       "      <td>[sociedad, andalucía, provinciasevilla, aceite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Madrid activa las videoconsultas y las altas p...</td>\n",
       "      <td>[comunidaddemadrid, madrid, sanidad, sanidadpú...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Un tercio de los cuarentañeros ya ha recibido ...</td>\n",
       "      <td>[coronaviruscovid19, vacunación, vacunas, coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Las incógnitas de la covid persistente en niño...</td>\n",
       "      <td>[sociedad, coronavirus, coronaviruscovid19, ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Las UCI están al límite de su capacidad estruc...</td>\n",
       "      <td>[coronavirus, coronaviruscovid19, enfermedades...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Todavía no es una gripe\\n\\nLa incertidumbre qu...</td>\n",
       "      <td>[opinión, coronaviruscovid19, sanidad, gobiern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Los casos de covid en las residencias se dupli...</td>\n",
       "      <td>[coronaviruscovid19, residenciasancianos, cata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vídeo | Cronología del ‘caso Djokovic’\\n\\nEl e...</td>\n",
       "      <td>[novakdjokovic, australia, openaustralia, teni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‘Palmas’\\n\\nMacarena Smerdou simboliza con las...</td>\n",
       "      <td>[madrid, cultura, arte, premios, sanidad, coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‘Mascarillas que son corazones’\\n\\nHéctor Delg...</td>\n",
       "      <td>[madrid, premios, arte, pandemia, coronavirus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>‘El universo’\\n\\nLa ilustradora Sara Lozano, c...</td>\n",
       "      <td>[madrid, arte, premios, ayuntamientomadrid, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>‘Héroes anónimos’\\n\\nLa ilustradora Susan Suth...</td>\n",
       "      <td>[madrid, arte, premios, ayuntamientomadrid, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>‘¡Gracias a nuestros héroes!’\\n\\nAndrea Wizner...</td>\n",
       "      <td>[arte, premios, cultura, coronavirus, sanidad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>‘Universal’\\n\\nLa diseñadora María Ángeles Car...</td>\n",
       "      <td>[madrid, arte, premios, dibujantes, ayuntamien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>La odisea de recuperar el suelo pélvico tras e...</td>\n",
       "      <td>[niños, padres, madres, hijos, parentesco, fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Los datos de hospitalizados con el coronavirus...</td>\n",
       "      <td>[coronavirus, coronaviruscovid19, enfermedades...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>“La reproducción 3D cambiará la docencia clíni...</td>\n",
       "      <td>[cataluña, hospitalsantjoandedeu, barcelona, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>‘Atardecer’\\n\\nLa estudiante Laura Bueno refle...</td>\n",
       "      <td>[madrid, arte, premios, ayuntamientomadrid, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>‘Enfoque láser’\\n\\nLa ilustradora Adela Trifan...</td>\n",
       "      <td>[madrid, arte, premios, ayuntamientomadrid, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>‘¡Gracias!’\\n\\nEl diseñador gráfico Alberto Ma...</td>\n",
       "      <td>[arte, madrid, cultura, premios, ayuntamientom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Una mujer supera una de las peores infecciones...</td>\n",
       "      <td>[ciencia, infecciones, bacteria, medicina, sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>El Gobierno desvela su reforma legal contra la...</td>\n",
       "      <td>[ciencia, dianamorantripoll, crisisdelaciencia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>La erupción del volcán de Tonga es la mayor re...</td>\n",
       "      <td>[ciencia, erupcionesvolcanes, volcanes, tonga,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Endemia, al fin\\n\\nSuperar la pandemia no será...</td>\n",
       "      <td>[ciencia, coronavirus, pandemia, infecciones, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Invitados indeseables por Navidad: el muérdago...</td>\n",
       "      <td>[ciencia, nutrición, alimentación, parasitolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>La ciencia en el centro de la agenda\\n\\nLa Asa...</td>\n",
       "      <td>[ciencia, políticacientífica, pandemia, saludp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>¿Somos polvo de estrellas?\\n\\nEs posible que l...</td>\n",
       "      <td>[ciencia, universo, bigbang, estrellas, astrof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Entendiendo el universo desde el sofá una tard...</td>\n",
       "      <td>[ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>“Para mí está muy claro que los neandertales h...</td>\n",
       "      <td>[ciencia, paleontología, rae, lengua, hombrede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>¿Qué hace a los humanos más altos o más bajos?...</td>\n",
       "      <td>[infancia, desarrolloinfantil, niños, salud, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Un estudio en Sudáfrica sugiere que ómicron so...</td>\n",
       "      <td>[ciencia, coronaviruscovid19, coronavirus, gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Un macroestudio señala al virus de la enfermed...</td>\n",
       "      <td>[ciencia, esclerosismúltiple, salud, mononucle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Sucesiones no numéricas\\n\\nLa naturaleza, la h...</td>\n",
       "      <td>[ciencia, matemáticas, carlofrabetti, juego, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>El laboratorio genético del aguacate \\n\\nTras ...</td>\n",
       "      <td>[ciencia, biología, genética, málaga, andalucí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Xi Jinping y la caja de Pandora de las tutoría...</td>\n",
       "      <td>[educación, china, enseñanzaprivada, gastoesco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Un jabalí de pezuñas hendidas\\n\\nEl modelo ind...</td>\n",
       "      <td>[ciencia, macrogranjasporcinas, consumo, alber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>La jerarquía católica española ante los abusos...</td>\n",
       "      <td>[sociedad, pederastia, iglesiacatólicaespañola...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Cuatro incógnitas sobre trasplantes de animale...</td>\n",
       "      <td>[ciencia, corazónaquino, animales, enfermedade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Entendiendo el universo desde el sofá una tard...</td>\n",
       "      <td>[ciencia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Matemáticas para hacer frente a los retos de l...</td>\n",
       "      <td>[ciencia, genomahumano, matemáticas, investiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Microsoft compra por 60.000 millones la empres...</td>\n",
       "      <td>[economía, microsoft, activision, software, vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>La compra de Activision por Microsoft: como si...</td>\n",
       "      <td>[tecnología, microsoft, activision, operacione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Los mapas que predijeron la pandemia y ayudaro...</td>\n",
       "      <td>[tecnología, ciencia, mapas, cartografía, coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Twitter incluye a España en su experimento par...</td>\n",
       "      <td>[tecnología]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Captadas en redes sociales, controladas por el...</td>\n",
       "      <td>[sociedad, prostitución, explotaciónsexual, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Radar Covid no ha funcionado. Pero la tecnolog...</td>\n",
       "      <td>[tecnología, apps, radarcovid, coronaviruscovi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>La CNMV podrá vetar la publicidad masiva sobre...</td>\n",
       "      <td>[economía, bitcoin, inversión, fraudefiscal, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>‘Influencers’: ¿explotados o explotadores?\\n\\n...</td>\n",
       "      <td>[redessociales, internet, instagram, famosos, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>‘Apps’ contra la violencia machista: ni proble...</td>\n",
       "      <td>[apps, mujeres, seguridadpersonal, privacidadi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Expulsados de Tinder o ‘Fortnite’ de por vida ...</td>\n",
       "      <td>[tecnología, redessociales, tinder, tecnología...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>El algoritmo se equivoca. La cara no siempre d...</td>\n",
       "      <td>[tecnología, transformacióndigital, reconocimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Movilización por una goma verde\\n\\nSomos senti...</td>\n",
       "      <td>[opinión, egb, twitter, redessociales, televis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>La crisis de Kazajistán infla los sueños de bi...</td>\n",
       "      <td>[elsalvador, nayibbukele, bitcoin, tecnología,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Suplantación de perfiles: ‘‘Con mis fotos de I...</td>\n",
       "      <td>[tecnología, redessociales, instagram, twitter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>¿Te han robado el móvil? Los pasos para poder ...</td>\n",
       "      <td>[tecnología, telefoníamóvil, smartphone, robos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Recetas para la reindustrialización: innovació...</td>\n",
       "      <td>[tecnología, industria, digitalizaciónempresar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>El futuro de la industria en Series Retina\\n\\n...</td>\n",
       "      <td>[tecnología, industria, españa, digitalización...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Ideas para descarbonizar la economía\\n\\nCooper...</td>\n",
       "      <td>[tecnología, medioambiente, cumbredelclima, en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>La descarbonización de la economía, en los eve...</td>\n",
       "      <td>[tecnología, medioambiente, ecología, energía,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 data  \\\n",
       "0   La trama del falso oro verde: desmantelada una...   \n",
       "1   Madrid activa las videoconsultas y las altas p...   \n",
       "2   Un tercio de los cuarentañeros ya ha recibido ...   \n",
       "3   Las incógnitas de la covid persistente en niño...   \n",
       "4   Las UCI están al límite de su capacidad estruc...   \n",
       "5   Todavía no es una gripe\\n\\nLa incertidumbre qu...   \n",
       "6   Los casos de covid en las residencias se dupli...   \n",
       "7   Vídeo | Cronología del ‘caso Djokovic’\\n\\nEl e...   \n",
       "8   ‘Palmas’\\n\\nMacarena Smerdou simboliza con las...   \n",
       "9   ‘Mascarillas que son corazones’\\n\\nHéctor Delg...   \n",
       "10  ‘El universo’\\n\\nLa ilustradora Sara Lozano, c...   \n",
       "11  ‘Héroes anónimos’\\n\\nLa ilustradora Susan Suth...   \n",
       "12  ‘¡Gracias a nuestros héroes!’\\n\\nAndrea Wizner...   \n",
       "13  ‘Universal’\\n\\nLa diseñadora María Ángeles Car...   \n",
       "14  La odisea de recuperar el suelo pélvico tras e...   \n",
       "15  Los datos de hospitalizados con el coronavirus...   \n",
       "16  “La reproducción 3D cambiará la docencia clíni...   \n",
       "17  ‘Atardecer’\\n\\nLa estudiante Laura Bueno refle...   \n",
       "18  ‘Enfoque láser’\\n\\nLa ilustradora Adela Trifan...   \n",
       "19  ‘¡Gracias!’\\n\\nEl diseñador gráfico Alberto Ma...   \n",
       "20  Una mujer supera una de las peores infecciones...   \n",
       "21  El Gobierno desvela su reforma legal contra la...   \n",
       "22  La erupción del volcán de Tonga es la mayor re...   \n",
       "23  Endemia, al fin\\n\\nSuperar la pandemia no será...   \n",
       "24  Invitados indeseables por Navidad: el muérdago...   \n",
       "25  La ciencia en el centro de la agenda\\n\\nLa Asa...   \n",
       "26  ¿Somos polvo de estrellas?\\n\\nEs posible que l...   \n",
       "27  Entendiendo el universo desde el sofá una tard...   \n",
       "28  “Para mí está muy claro que los neandertales h...   \n",
       "29  ¿Qué hace a los humanos más altos o más bajos?...   \n",
       "30  Un estudio en Sudáfrica sugiere que ómicron so...   \n",
       "31  Un macroestudio señala al virus de la enfermed...   \n",
       "32  Sucesiones no numéricas\\n\\nLa naturaleza, la h...   \n",
       "33  El laboratorio genético del aguacate \\n\\nTras ...   \n",
       "34  Xi Jinping y la caja de Pandora de las tutoría...   \n",
       "35  Un jabalí de pezuñas hendidas\\n\\nEl modelo ind...   \n",
       "36  La jerarquía católica española ante los abusos...   \n",
       "37  Cuatro incógnitas sobre trasplantes de animale...   \n",
       "38  Entendiendo el universo desde el sofá una tard...   \n",
       "39  Matemáticas para hacer frente a los retos de l...   \n",
       "40  Microsoft compra por 60.000 millones la empres...   \n",
       "41  La compra de Activision por Microsoft: como si...   \n",
       "42  Los mapas que predijeron la pandemia y ayudaro...   \n",
       "43  Twitter incluye a España en su experimento par...   \n",
       "44  Captadas en redes sociales, controladas por el...   \n",
       "45  Radar Covid no ha funcionado. Pero la tecnolog...   \n",
       "46  La CNMV podrá vetar la publicidad masiva sobre...   \n",
       "47  ‘Influencers’: ¿explotados o explotadores?\\n\\n...   \n",
       "48  ‘Apps’ contra la violencia machista: ni proble...   \n",
       "49  Expulsados de Tinder o ‘Fortnite’ de por vida ...   \n",
       "50  El algoritmo se equivoca. La cara no siempre d...   \n",
       "51  Movilización por una goma verde\\n\\nSomos senti...   \n",
       "52  La crisis de Kazajistán infla los sueños de bi...   \n",
       "53  Suplantación de perfiles: ‘‘Con mis fotos de I...   \n",
       "54  ¿Te han robado el móvil? Los pasos para poder ...   \n",
       "56  Recetas para la reindustrialización: innovació...   \n",
       "57  El futuro de la industria en Series Retina\\n\\n...   \n",
       "58  Ideas para descarbonizar la economía\\n\\nCooper...   \n",
       "59  La descarbonización de la economía, en los eve...   \n",
       "\n",
       "                                            etiquetas  \n",
       "0   [sociedad, andalucía, provinciasevilla, aceite...  \n",
       "1   [comunidaddemadrid, madrid, sanidad, sanidadpú...  \n",
       "2   [coronaviruscovid19, vacunación, vacunas, coro...  \n",
       "3   [sociedad, coronavirus, coronaviruscovid19, ni...  \n",
       "4   [coronavirus, coronaviruscovid19, enfermedades...  \n",
       "5   [opinión, coronaviruscovid19, sanidad, gobiern...  \n",
       "6   [coronaviruscovid19, residenciasancianos, cata...  \n",
       "7   [novakdjokovic, australia, openaustralia, teni...  \n",
       "8   [madrid, cultura, arte, premios, sanidad, coro...  \n",
       "9   [madrid, premios, arte, pandemia, coronavirus,...  \n",
       "10  [madrid, arte, premios, ayuntamientomadrid, pa...  \n",
       "11  [madrid, arte, premios, ayuntamientomadrid, di...  \n",
       "12  [arte, premios, cultura, coronavirus, sanidad,...  \n",
       "13  [madrid, arte, premios, dibujantes, ayuntamien...  \n",
       "14  [niños, padres, madres, hijos, parentesco, fam...  \n",
       "15  [coronavirus, coronaviruscovid19, enfermedades...  \n",
       "16  [cataluña, hospitalsantjoandedeu, barcelona, h...  \n",
       "17  [madrid, arte, premios, ayuntamientomadrid, di...  \n",
       "18  [madrid, arte, premios, ayuntamientomadrid, sa...  \n",
       "19  [arte, madrid, cultura, premios, ayuntamientom...  \n",
       "20  [ciencia, infecciones, bacteria, medicina, sal...  \n",
       "21  [ciencia, dianamorantripoll, crisisdelaciencia...  \n",
       "22  [ciencia, erupcionesvolcanes, volcanes, tonga,...  \n",
       "23  [ciencia, coronavirus, pandemia, infecciones, ...  \n",
       "24  [ciencia, nutrición, alimentación, parasitolog...  \n",
       "25  [ciencia, políticacientífica, pandemia, saludp...  \n",
       "26  [ciencia, universo, bigbang, estrellas, astrof...  \n",
       "27                                          [ciencia]  \n",
       "28  [ciencia, paleontología, rae, lengua, hombrede...  \n",
       "29  [infancia, desarrolloinfantil, niños, salud, c...  \n",
       "30  [ciencia, coronaviruscovid19, coronavirus, gen...  \n",
       "31  [ciencia, esclerosismúltiple, salud, mononucle...  \n",
       "32  [ciencia, matemáticas, carlofrabetti, juego, p...  \n",
       "33  [ciencia, biología, genética, málaga, andalucí...  \n",
       "34  [educación, china, enseñanzaprivada, gastoesco...  \n",
       "35  [ciencia, macrogranjasporcinas, consumo, alber...  \n",
       "36  [sociedad, pederastia, iglesiacatólicaespañola...  \n",
       "37  [ciencia, corazónaquino, animales, enfermedade...  \n",
       "38                                          [ciencia]  \n",
       "39  [ciencia, genomahumano, matemáticas, investiga...  \n",
       "40  [economía, microsoft, activision, software, vi...  \n",
       "41  [tecnología, microsoft, activision, operacione...  \n",
       "42  [tecnología, ciencia, mapas, cartografía, coro...  \n",
       "43                                       [tecnología]  \n",
       "44  [sociedad, prostitución, explotaciónsexual, tr...  \n",
       "45  [tecnología, apps, radarcovid, coronaviruscovi...  \n",
       "46  [economía, bitcoin, inversión, fraudefiscal, c...  \n",
       "47  [redessociales, internet, instagram, famosos, ...  \n",
       "48  [apps, mujeres, seguridadpersonal, privacidadi...  \n",
       "49  [tecnología, redessociales, tinder, tecnología...  \n",
       "50  [tecnología, transformacióndigital, reconocimi...  \n",
       "51  [opinión, egb, twitter, redessociales, televis...  \n",
       "52  [elsalvador, nayibbukele, bitcoin, tecnología,...  \n",
       "53  [tecnología, redessociales, instagram, twitter...  \n",
       "54  [tecnología, telefoníamóvil, smartphone, robos...  \n",
       "56  [tecnología, industria, digitalizaciónempresar...  \n",
       "57  [tecnología, industria, españa, digitalización...  \n",
       "58  [tecnología, medioambiente, cumbredelclima, en...  \n",
       "59  [tecnología, medioambiente, ecología, energía,...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-ready",
   "metadata": {},
   "source": [
    "Este método nos devuelve un dataframe con una columna con el texto de la noticia y otra con las etiquetas de la noticia de El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "animated-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoEtiquetasMinutos(arraySalud, arrayCiencia, arrayTecnologia, df): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "#------------SALUD-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arraySalud[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            texto = titulo+\"\\n\\n\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        texto = texto+str(each.get_text())\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            texto = texto+\"\\n\"\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    texto = texto+cont.get_text()\n",
    "                except:\n",
    "                    texto = texto+\"\"\n",
    "            \n",
    "            #sacamos las etiquetas\n",
    "            li = soup.find_all('li', class_=\"tag\")\n",
    "            etiquetas = []\n",
    "            for each in li:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "                \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "        \n",
    "#------------CIENCIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayCiencia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            texto = titulo+\"\\n\\n\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        texto = texto+str(each.get_text())\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            texto = texto+\"\\n\"\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    texto = texto+cont.get_text()\n",
    "                except:\n",
    "                    texto = texto+\"\"\n",
    "            \n",
    "            #sacamos las etiquetas\n",
    "            li = soup.find_all('li', class_=\"tag\")\n",
    "            etiquetas = []\n",
    "            for each in li:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "                \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "#------------TECNOLOGIA-------------\n",
    "    for i in range(20):\n",
    "        req = requests.get(arrayTecnologia[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            texto = titulo+\"\\n\\n\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        texto = texto+str(each.get_text())\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            texto = texto+\"\\n\"\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    texto = texto+cont.get_text()\n",
    "                except:\n",
    "                    texto = texto+\"\"\n",
    "            \n",
    "            #sacamos las etiquetas\n",
    "            li = soup.find_all('li', class_=\"tag\")\n",
    "            etiquetas = []\n",
    "            for each in li:\n",
    "                etiquetas.append(each.get_text())\n",
    "                for i in range(len(etiquetas)):\n",
    "                    etiquetas[i] = re.sub(\"\\ |\\\\n|\\-\",\"\",etiquetas[i])\n",
    "                    etiquetas[i] = etiquetas[i].lower()\n",
    "                \n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'data':texto,'etiquetas': etiquetas}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1\n",
    "\n",
    "    df.drop(df.loc[df['etiquetas']==''].index, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "inclusive-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Esto es lo que pasa en tu cuerpo cuando fumas ...</td>\n",
       "      <td>[tabaco, monóxidodecarbono, oms, cáncerdepulmó...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por qué es bueno para la salud de los niños en...</td>\n",
       "      <td>[salud, alcohol, adolescentes, niños]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Características, síntomas y complicaciones del...</td>\n",
       "      <td>[salud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\nSi llevas una chaqueta o un abrigo, quít...</td>\n",
       "      <td>[perros, invierno, salud, fuego, apagón]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El aditivo alimentario E171 con dióxido de tit...</td>\n",
       "      <td>[unióneuropea, salud, alimentación, alimentari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Un avance frente al cáncer, el ictus o la insu...</td>\n",
       "      <td>[cáncer, ictus, crispr, células, covid19, tipo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Un avance frente al cáncer, el ictus o la insu...</td>\n",
       "      <td>[cáncer, ictus, crispr, células, covid19, tipo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Un conocido epidemiólogo de EEUU predice qué p...</td>\n",
       "      <td>[eeuu, covid19, ómicronvariantesudafricanacovid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Un conocido epidemiólogo de EEUU predice qué p...</td>\n",
       "      <td>[eeuu, covid19, ómicronvariantesudafricanacovid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cuándo pierde la covid su efectividad para con...</td>\n",
       "      <td>[covid19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cuándo pierde la covid su efectividad para con...</td>\n",
       "      <td>[covid19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Un equipo médico de Huelva logra combatir con ...</td>\n",
       "      <td>[depresión, antidepresivos, psicólogos, enferm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Un equipo médico de Huelva logra combatir con ...</td>\n",
       "      <td>[depresión, antidepresivos, psicólogos, enferm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Animales vs máquinas: cuáles son las ventajas ...</td>\n",
       "      <td>[trasplante, médicos, corazón]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Animales vs máquinas: cuáles son las ventajas ...</td>\n",
       "      <td>[trasplante, médicos, corazón]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Características, síntomas y complicaciones del...</td>\n",
       "      <td>[salud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Características, síntomas y complicaciones del...</td>\n",
       "      <td>[salud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>¿Qué es la lipodistrofia? Estas son sus causas...</td>\n",
       "      <td>[diabetes, colesterol, insulina, enfermedades,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>¿Qué es la lipodistrofia? Estas son sus causas...</td>\n",
       "      <td>[diabetes, colesterol, insulina, enfermedades,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>¿Qué es la apendicitis? Estos son sus síntomas...</td>\n",
       "      <td>[fiebre, estómago, síntomas, medicina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>El efecto 'nocebo' en las vacunas: algunos sín...</td>\n",
       "      <td>[eeuu, vacunas, placebo, síntomas, covid19, pf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Qué posibilidades hay de que el asteroide pote...</td>\n",
       "      <td>[nasa, latierra, asteroides]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Horario y cómo ver el asteroide potencialmente...</td>\n",
       "      <td>[nasa, asteroides]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>El último descubrimiento del 'Curiosity' de la...</td>\n",
       "      <td>[house, nasa, marte, sistemasolar, co2, curios...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>El asteroide Kamo`oalewa podría ser resultado ...</td>\n",
       "      <td>[sol, asteroides]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>La cuarta dosis de la vacuna genera más anticu...</td>\n",
       "      <td>[vacunas, centromédico, covid19, pfizer, antic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Moderna tendría lista en 2023 una vacuna única...</td>\n",
       "      <td>[vacunas, gripe, foroeconómicomundialdedavos, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>¿Por qué unas personas pierden el olfato y el ...</td>\n",
       "      <td>[adn, síntomas, covid19, ómicronvariantesudafr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>La acción humana afecta negativamente a las av...</td>\n",
       "      <td>[cambioclimático, doñana, csic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cómo ver la luna de lobo de enero 2022: a qué ...</td>\n",
       "      <td>[luna, astronomía, lobo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Cómo conocer la edad real de tu cuerpo con est...</td>\n",
       "      <td>[matemáticas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Los inmunólogos advierten de la estrategia act...</td>\n",
       "      <td>[vacunas, coronavirus, covid19, vacunacoronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>De dónde procede el oxígeno de las máscaras de...</td>\n",
       "      <td>[avión, cienciaparatodos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>La EMA incluye una rara enfermedad de la médul...</td>\n",
       "      <td>[coronavirus, coronavirusdewuhan, covid19, ast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>El calor interior de la Tierra se disipa más r...</td>\n",
       "      <td>[latierra, calor, temperaturas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Un estudio explica las diferencias entre ómicr...</td>\n",
       "      <td>[coronavirus, coronavirusdewuhan, covid19, ómi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Hay un 1% de posibilidades de que un asteroide...</td>\n",
       "      <td>[simo, universitatdevalencia, refugiados, polí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Llega la luna del lobo: la primera luna llena ...</td>\n",
       "      <td>[luna, astronomía]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>La curiosa forma en que los perros saben la ho...</td>\n",
       "      <td>[perros]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Los neurólogos aclaran que haber sufrido monon...</td>\n",
       "      <td>[esclerosismúltiple, medicina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Los costes humanos de la tecnología: los probl...</td>\n",
       "      <td>[amazon, comercioelectrónico, internet, amazon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Microsoft gasta más de 60.000 millones de euro...</td>\n",
       "      <td>[microsoft, pc, blizzard, esports, xbox, overw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Los daños en los cables submarinos de Tonga de...</td>\n",
       "      <td>[internet, tsunami, volcán, noticiasdetecnología]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Amazon seguirá aceptando pagos con tarjetas de...</td>\n",
       "      <td>[reinounido, amazon, noticiasdetecnología]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Los drones se convierten en un arma de guerra ...</td>\n",
       "      <td>[méxico, drones, armas, móvilesydispositivos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Un yate para 22 personas que vuela como una ae...</td>\n",
       "      <td>[yates, noticiasdetecnología]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>'Fresh Air Clip', un wearable que detecta si h...</td>\n",
       "      <td>[gadgets, coronavirus, covid19, móvilesydispos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ZTE Axon 30 Ultra: un regreso a lo grande\\n\\nN...</td>\n",
       "      <td>[china, teléfonomóvil, zte, tech, reviews]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Hisense lanza la primera pantalla láser 8K del...</td>\n",
       "      <td>[móvilesydispositivos, televisores]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>No te dejes llevar por las rebajas: por qué en...</td>\n",
       "      <td>[teléfonomóvil, móvilesydispositivos, crisisde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>iPad 2021: la tablet más asequible de Apple su...</td>\n",
       "      <td>[apple, ipad, tablet, keynotedeapple, móvilesy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Huawei Watch Fit mini: diseño atemporal que pe...</td>\n",
       "      <td>[salud, huawei, fitness, móvilesydispositivos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Panasonic Technics EAH-AZ60: despídete de los ...</td>\n",
       "      <td>[panasonic, sonido, móvilesydispositivos, revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>POCO M4 Pro 5G: diseño cautivador para un móvi...</td>\n",
       "      <td>[teléfonomóvil, xiaomi, 5g, móvilesydispositiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Google Nest Cam: vigilar tu casa nunca fue tan...</td>\n",
       "      <td>[google, internet, gadgets, inteligenciaartifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Un error del navegador Safari filtra el histor...</td>\n",
       "      <td>[google, apple, seguridadinformática, internet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Rusia desmantela a REvil, la banda de cibercri...</td>\n",
       "      <td>[ciberseguridad, ransomware, ciberataque]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Apple confirma que estos modelos de iPhone se ...</td>\n",
       "      <td>[iphone, apple, ios, móvilesydispositivos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Estas aplicaciones de Android dejarán de tener...</td>\n",
       "      <td>[blackberry, android, móvilesydispositivos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>WhatsApp está desarrollando una herramienta pa...</td>\n",
       "      <td>[mensajeríainstantánea, chat, whatsapp, aplica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 data  \\\n",
       "0   Esto es lo que pasa en tu cuerpo cuando fumas ...   \n",
       "1   Por qué es bueno para la salud de los niños en...   \n",
       "2   Características, síntomas y complicaciones del...   \n",
       "3   \\n\\n\\nSi llevas una chaqueta o un abrigo, quít...   \n",
       "4   El aditivo alimentario E171 con dióxido de tit...   \n",
       "5   Un avance frente al cáncer, el ictus o la insu...   \n",
       "6   Un avance frente al cáncer, el ictus o la insu...   \n",
       "7   Un conocido epidemiólogo de EEUU predice qué p...   \n",
       "8   Un conocido epidemiólogo de EEUU predice qué p...   \n",
       "9   Cuándo pierde la covid su efectividad para con...   \n",
       "10  Cuándo pierde la covid su efectividad para con...   \n",
       "11  Un equipo médico de Huelva logra combatir con ...   \n",
       "12  Un equipo médico de Huelva logra combatir con ...   \n",
       "13  Animales vs máquinas: cuáles son las ventajas ...   \n",
       "14  Animales vs máquinas: cuáles son las ventajas ...   \n",
       "15  Características, síntomas y complicaciones del...   \n",
       "16  Características, síntomas y complicaciones del...   \n",
       "17  ¿Qué es la lipodistrofia? Estas son sus causas...   \n",
       "18  ¿Qué es la lipodistrofia? Estas son sus causas...   \n",
       "19  ¿Qué es la apendicitis? Estos son sus síntomas...   \n",
       "20  El efecto 'nocebo' en las vacunas: algunos sín...   \n",
       "21  Qué posibilidades hay de que el asteroide pote...   \n",
       "22  Horario y cómo ver el asteroide potencialmente...   \n",
       "23  El último descubrimiento del 'Curiosity' de la...   \n",
       "24  El asteroide Kamo`oalewa podría ser resultado ...   \n",
       "25  La cuarta dosis de la vacuna genera más anticu...   \n",
       "26  Moderna tendría lista en 2023 una vacuna única...   \n",
       "27  ¿Por qué unas personas pierden el olfato y el ...   \n",
       "28  La acción humana afecta negativamente a las av...   \n",
       "29  Cómo ver la luna de lobo de enero 2022: a qué ...   \n",
       "30  Cómo conocer la edad real de tu cuerpo con est...   \n",
       "31  Los inmunólogos advierten de la estrategia act...   \n",
       "32  De dónde procede el oxígeno de las máscaras de...   \n",
       "33  La EMA incluye una rara enfermedad de la médul...   \n",
       "34  El calor interior de la Tierra se disipa más r...   \n",
       "35  Un estudio explica las diferencias entre ómicr...   \n",
       "36  Hay un 1% de posibilidades de que un asteroide...   \n",
       "37  Llega la luna del lobo: la primera luna llena ...   \n",
       "38  La curiosa forma en que los perros saben la ho...   \n",
       "39  Los neurólogos aclaran que haber sufrido monon...   \n",
       "40  Los costes humanos de la tecnología: los probl...   \n",
       "41  Microsoft gasta más de 60.000 millones de euro...   \n",
       "42  Los daños en los cables submarinos de Tonga de...   \n",
       "43  Amazon seguirá aceptando pagos con tarjetas de...   \n",
       "44  Los drones se convierten en un arma de guerra ...   \n",
       "45  Un yate para 22 personas que vuela como una ae...   \n",
       "46  'Fresh Air Clip', un wearable que detecta si h...   \n",
       "47  ZTE Axon 30 Ultra: un regreso a lo grande\\n\\nN...   \n",
       "48  Hisense lanza la primera pantalla láser 8K del...   \n",
       "49  No te dejes llevar por las rebajas: por qué en...   \n",
       "50  iPad 2021: la tablet más asequible de Apple su...   \n",
       "51  Huawei Watch Fit mini: diseño atemporal que pe...   \n",
       "52  Panasonic Technics EAH-AZ60: despídete de los ...   \n",
       "53  POCO M4 Pro 5G: diseño cautivador para un móvi...   \n",
       "54  Google Nest Cam: vigilar tu casa nunca fue tan...   \n",
       "55  Un error del navegador Safari filtra el histor...   \n",
       "56  Rusia desmantela a REvil, la banda de cibercri...   \n",
       "57  Apple confirma que estos modelos de iPhone se ...   \n",
       "58  Estas aplicaciones de Android dejarán de tener...   \n",
       "59  WhatsApp está desarrollando una herramienta pa...   \n",
       "\n",
       "                                            etiquetas  \n",
       "0   [tabaco, monóxidodecarbono, oms, cáncerdepulmó...  \n",
       "1               [salud, alcohol, adolescentes, niños]  \n",
       "2                                             [salud]  \n",
       "3            [perros, invierno, salud, fuego, apagón]  \n",
       "4   [unióneuropea, salud, alimentación, alimentari...  \n",
       "5   [cáncer, ictus, crispr, células, covid19, tipo...  \n",
       "6   [cáncer, ictus, crispr, células, covid19, tipo...  \n",
       "7    [eeuu, covid19, ómicronvariantesudafricanacovid]  \n",
       "8    [eeuu, covid19, ómicronvariantesudafricanacovid]  \n",
       "9                                           [covid19]  \n",
       "10                                          [covid19]  \n",
       "11  [depresión, antidepresivos, psicólogos, enferm...  \n",
       "12  [depresión, antidepresivos, psicólogos, enferm...  \n",
       "13                     [trasplante, médicos, corazón]  \n",
       "14                     [trasplante, médicos, corazón]  \n",
       "15                                            [salud]  \n",
       "16                                            [salud]  \n",
       "17  [diabetes, colesterol, insulina, enfermedades,...  \n",
       "18  [diabetes, colesterol, insulina, enfermedades,...  \n",
       "19             [fiebre, estómago, síntomas, medicina]  \n",
       "20  [eeuu, vacunas, placebo, síntomas, covid19, pf...  \n",
       "21                       [nasa, latierra, asteroides]  \n",
       "22                                 [nasa, asteroides]  \n",
       "23  [house, nasa, marte, sistemasolar, co2, curios...  \n",
       "24                                  [sol, asteroides]  \n",
       "25  [vacunas, centromédico, covid19, pfizer, antic...  \n",
       "26  [vacunas, gripe, foroeconómicomundialdedavos, ...  \n",
       "27  [adn, síntomas, covid19, ómicronvariantesudafr...  \n",
       "28                    [cambioclimático, doñana, csic]  \n",
       "29                           [luna, astronomía, lobo]  \n",
       "30                                      [matemáticas]  \n",
       "31  [vacunas, coronavirus, covid19, vacunacoronavi...  \n",
       "32                          [avión, cienciaparatodos]  \n",
       "33  [coronavirus, coronavirusdewuhan, covid19, ast...  \n",
       "34                    [latierra, calor, temperaturas]  \n",
       "35  [coronavirus, coronavirusdewuhan, covid19, ómi...  \n",
       "36  [simo, universitatdevalencia, refugiados, polí...  \n",
       "37                                 [luna, astronomía]  \n",
       "38                                           [perros]  \n",
       "39                     [esclerosismúltiple, medicina]  \n",
       "40  [amazon, comercioelectrónico, internet, amazon...  \n",
       "41  [microsoft, pc, blizzard, esports, xbox, overw...  \n",
       "42  [internet, tsunami, volcán, noticiasdetecnología]  \n",
       "43         [reinounido, amazon, noticiasdetecnología]  \n",
       "44      [méxico, drones, armas, móvilesydispositivos]  \n",
       "45                      [yates, noticiasdetecnología]  \n",
       "46  [gadgets, coronavirus, covid19, móvilesydispos...  \n",
       "47         [china, teléfonomóvil, zte, tech, reviews]  \n",
       "48                [móvilesydispositivos, televisores]  \n",
       "49  [teléfonomóvil, móvilesydispositivos, crisisde...  \n",
       "50  [apple, ipad, tablet, keynotedeapple, móvilesy...  \n",
       "51  [salud, huawei, fitness, móvilesydispositivos,...  \n",
       "52  [panasonic, sonido, móvilesydispositivos, revi...  \n",
       "53  [teléfonomóvil, xiaomi, 5g, móvilesydispositiv...  \n",
       "54  [google, internet, gadgets, inteligenciaartifi...  \n",
       "55  [google, apple, seguridadinformática, internet...  \n",
       "56          [ciberseguridad, ransomware, ciberataque]  \n",
       "57         [iphone, apple, ios, móvilesydispositivos]  \n",
       "58        [blackberry, android, móvilesydispositivos]  \n",
       "59  [mensajeríainstantánea, chat, whatsapp, aplica...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = ['data','etiquetas'])\n",
    "df = contenidoEtiquetasMinutos(linksMinutosSalud, linksMinutosCiencia, linksMinutosTecnologia, df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://www.20minutos.es/salud/la-ema-confirma-la-seguridad-de-las-vacunas-de-arnm-en-embarazadas-4943023/\")\n",
    "soup = BeautifulSoup (req.content, 'html.parser')\n",
    "li = soup.find_all('li', class_=\"tag\")\n",
    "etiquetas = []\n",
    "for each in li:\n",
    "    etiquetas.append(each.get_text())\n",
    "    import re\n",
    "    for i in range(len(etiquetas)):\n",
    "        etiquetas[i] = re.sub(\"\\ |\\\\n\",\"\",etiquetas[i])\n",
    "        etiquetas[i] = etiquetas[i].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-receptor",
   "metadata": {},
   "source": [
    "# 3.) Tratamiento de las noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-chest",
   "metadata": {},
   "source": [
    "Este método guardamos en un dataframe \"df\" los diferentes ficheros de cada tematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia,ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia,ruta_PaisCiencia,ruta_PaisSalud,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSalud = os.listdir(ruta_PaisSalud)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_20Mins(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "\n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "\n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_ElMundo(ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia):\n",
    "    import os\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "\n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias_ElPais(ruta_PaisCiencia,ruta_PaisSalud,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSalud = os.listdir(ruta_PaisSalud)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-iceland",
   "metadata": {},
   "source": [
    "Este metodo compara una noticia con el resto dentro del df y devuelve un dataframe con cada texto y la similitud que tiene con el texto introducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONA\n",
    "def compararNoticiasActualzado(textoin, df):\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    newdf = pd.DataFrame(columns = ['porcentage','data'])\n",
    "    \n",
    "    f = open(\"stopwords.txt\")\n",
    "    mis_stopwords = f.read() \n",
    "    vectorizer = TfidfVectorizer(stop_words=set(mis_stopwords))\n",
    "\n",
    "    #vectorizer = TfidfVectorizer()\n",
    "    for i in range(len(df)):\n",
    "        X = vectorizer.fit_transform([textoin,df.data[i]])\n",
    "        similarity_matrix = cosine_similarity(X,X)\n",
    "        porcentage = float(similarity_matrix[1][0])\n",
    "        #print(porcentaje)\n",
    "        texto = df.data[i]\n",
    "        new_row = {'porcentage':porcentage,'data': texto}\n",
    "        newdf = newdf.append(new_row, ignore_index=True)\n",
    "        i+=1\n",
    "    \n",
    "    newdf = newdf.sort_values('porcentage',ascending=False)\n",
    "    newdf = newdf.reset_index()\n",
    "    newdf = newdf.drop(['index'], axis=1)\n",
    "    newdf = newdf.drop([0],axis=0)\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-meaning",
   "metadata": {},
   "source": [
    "# 4.) Ventana (GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CON LA VENTANA\n",
    "\n",
    "import os\n",
    "import PySimpleGUI as sg\n",
    "sg.theme('BlueMono')   # Add a little color to your windows\n",
    "# All the stuff inside your window. This is the PSG magic code compactor...\n",
    "\n",
    "listaNumero = [\"1\", \"2\", \"3\", \"4\", \"5\"] #numero de noticias que se quieren en el ranking\n",
    "list1 = [\"20 minutos\", \"El Pais\", \"El Mundo\", \"Todos\"] #Filtrar las noticias por el medio de origen o seleccionar todos\n",
    "params = 10\n",
    "medios = [\"20 minutos\", \"El Pais\", \"El Mundo\"]\n",
    "categorias = [\"Ciencia\", \"Salud\", \"Tecnologia\"]\n",
    "noticias = [\"noticia 1\", \"noticia 2\", \"noticia 3\", \"noticia 4\", \"noticia 5\", \"noticia 6\", \"noticia 7\"]\n",
    "noticiasPorcentajes = [\"\"]\n",
    "\n",
    "\n",
    "busqueda = [[sg.Text('Consulta:'), sg.InputText(size=80, key= 'textbox_buscar_noticia')],\n",
    "            [sg.Text('TOP-N:'),sg.Combo(listaNumero, size = params, key= 'combo_topN_consulta'),sg.Text('             '), sg.Text('Filtrar:'), sg.Combo(list1, size = params, key = 'combo_filtrar'),sg.Text('                              '), sg.Button('Buscar', size=10, key='btn_buscarNoticias')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('Ranking:'), sg.Button('Preview', key='btn_preview_busqueda'), sg.Text('                                                      '), sg.Text('Texto de la noticia:')],\n",
    "            [sg.Listbox(values=[], size=(30, 10), key='listbox_ranking_noticias'),sg.Text('             ') ,sg.Multiline(size = (50, 10), key = 'textbox_preview_noticia')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "comparar = [[sg.Text('Medio:'), sg.Text('                                  '), sg.Text('Categoría:'), sg.Text('                                                             '), sg.Text('Noticias:')],\n",
    "            [sg.Listbox(medios, size=(20, 4), key='listbox_medios'),sg.Text('     '), sg.Listbox(categorias, size=(20, 4), key='listbox_categorias'),sg.Text('     '), sg.Button(\"Ver\\nnoticias\", size=(10,2), key='btn_verNoticias'),sg.Text('     '), sg.Listbox(values=[], size=(30, 4), key='listbox_noticias')],\n",
    "            [sg.Button('Preview', key='btn_preview')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview')],\n",
    "            [sg.Text('                                      '), sg.Text('TOP-N:'), sg.Combo(listaNumero, size = params, key='combo_topN'),sg.Text('             '), sg.Button('Buscar', size=10, key='btn_buscar')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('                        '), sg.Text('Ranking:'), sg.Listbox(values=\"\", size=(30, 5), key='noticias_porcentajes'), sg.Button('Preview', key='btn_preview_reslutado')],\n",
    "            [sg.Text('Texto de la noticia:')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview_reslutado')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "\n",
    "recomendar = [[sg.Text('Medio:'), sg.Text('                                  '), sg.Text('Categoría:'), sg.Text('                                                             '), sg.Text('Noticias:')],\n",
    "            [sg.Listbox(medios, size=(20, 4), key='listbox_medios_recomendar'),sg.Text('     '), sg.Listbox(categorias, size=(20, 4), key='listbox_categorias_recomendar'),sg.Text('     '), sg.Button(\"Ver\\nnoticias\", size=(10,2), key='btn_verNoticias_recomendar'),sg.Text('     '), sg.Listbox(values=[], size=(30, 4), key='listbox_noticias_recomendar')],\n",
    "            [sg.Button('Preview', key='btn_preview_recomendar')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview_recomendar')],\n",
    "            [sg.Text('                                      '), sg.Text('TOP-N:'), sg.Combo(listaNumero, size = params, key='combo_topN_recomendar'),sg.Text('             '), sg.Button('Buscar', size=10, key='btn_buscar_recomendar')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('                        '), sg.Text('Filtrar:'), sg.Combo(medios, size = params, key=\"combo_filtro_recomendar\"),sg.Text('Ranking:'), sg.Listbox(values=\"\", size=(30, 5), key='noticias_porcentajes_recomendar'), sg.Button('Preview', key='btn_preview_reslutado_recomendar')],\n",
    "            [sg.Text('Texto de la noticia:')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview_reslutado_recomendar')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "\n",
    "tabgrp = [\n",
    "    [sg.TabGroup([[ sg.Tab('Comparador de noticias', comparar),\n",
    "                    sg.Tab('Buscador de noticias', busqueda),\n",
    "                    sg.Tab('Recomendador de noticias', recomendar)\n",
    "                ]], tab_location='top')]]\n",
    "\n",
    "# Create the Window\n",
    "window = sg.Window('Buscador y comparador de noticias', tabgrp)\n",
    "# Event Loop to process \"events\"\n",
    "while True:             \n",
    "    event, values = window.read()\n",
    "    if event in (sg.WIN_CLOSED, 'Salir'):\n",
    "        break\n",
    "\n",
    "    if event == \"btn_verNoticias\":\n",
    "        eleccion = values['listbox_medios']+values['listbox_categorias']\n",
    "        print(eleccion)\n",
    "        if(eleccion==['20 minutos', 'Ciencia']):\n",
    "            noticias = os.listdir(\"20Minutos/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['20 minutos', 'Salud']):\n",
    "            noticias = os.listdir(\"20Minutos/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['20 minutos', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"20Minutos/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "        elif(eleccion==['El Pais', 'Ciencia']):\n",
    "            noticias = os.listdir(\"ElPais/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Pais', 'Salud']):\n",
    "            noticias = os.listdir(\"ElPais/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Pais', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"ElPais/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "        elif(eleccion==['El Mundo', 'Ciencia']):\n",
    "            noticias = os.listdir(\"ElMundo/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Mundo', 'Salud']):\n",
    "            noticias = os.listdir(\"ElMundo/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Mundo', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"ElMundo/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "    if event==\"btn_preview\":\n",
    "        noticia = values['listbox_noticias']\n",
    "        f = open(eleccion[0].replace(\" \", \"\")+\"/\"+eleccion[1]+\"/\"+noticia[0], \"r\")\n",
    "        texto = f.read()\n",
    "        window['textbox_preview'].update(texto)\n",
    "        \n",
    "    if event==\"btn_buscar\":\n",
    "        eleccion = values['listbox_medios']+values['listbox_categorias']\n",
    "        noticia = values['listbox_noticias']\n",
    "        topN=values['combo_topN']\n",
    "        dirNoticia = open(eleccion[0].replace(\" \", \"\")+\"/\"+eleccion[1]+\"/\"+noticia[0], \"r\")\n",
    "        textoNoticia = dirNoticia.read()\n",
    "        df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "        dataRanking = compararNoticiasActualzado(textoNoticia,df)\n",
    "        rankingLimitado = []\n",
    "        textoRanking = []\n",
    "        for i in range(int(topN)):\n",
    "            i+=1\n",
    "            # REVISAR\n",
    "            rankingLimitado.append(str(int(dataRanking.porcentage[i]*100))+\" % de coincidencia\")\n",
    "            textoRanking.append(dataRanking.data[i])   \n",
    "\n",
    "        window.find_element('noticias_porcentajes').Update(values=rankingLimitado)\n",
    "        \n",
    "    if event==\"btn_preview_reslutado\":\n",
    "        textoResultado = \"\"        \n",
    "        for i in range(len(rankingLimitado)):\n",
    "            if str(values['noticias_porcentajes'][0])==str(rankingLimitado[i]):   \n",
    "                textoResultado = textoRanking[i]\n",
    "            i+=1\n",
    "        window['textbox_preview_reslutado'].update(textoResultado)\n",
    "    \n",
    "        #window['textbox_preview_noticia'].update(textoNoticia)\n",
    "        #window['textbox_preview_noticia'].update(textoNoticia)\n",
    "    \n",
    "    if event==\"btn_preview_busqueda\":\n",
    "        textoResultado = \"\"        \n",
    "        for i in range(len(rankingLimitado2)):\n",
    "            if str(values['listbox_ranking_noticias'][0])==str(rankingLimitado2[i]):   \n",
    "                textoResultado = textoRanking2[i]\n",
    "            i+=1\n",
    "        window['textbox_preview_noticia'].update(textoResultado)\n",
    "        \n",
    "    if event == \"btn_buscarNoticias\":\n",
    "        consulta = values['textbox_buscar_noticia']\n",
    "        topN=values['combo_topN_consulta']\n",
    "        filtro = values['combo_filtrar']\n",
    "        print(filtro)\n",
    "        with open('consulta.txt', 'w') as f:\n",
    "            f.write(consulta)\n",
    "        dirNoticia2 = open(\"consulta.txt\", \"r\")\n",
    "        textoConsulta = dirNoticia2.read()\n",
    "        #df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "        if(filtro=='20 minutos'):\n",
    "            df = cargar_noticias_20Mins(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='El Pais'):\n",
    "            df = cargar_noticias_ElPais(\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='El Mundo'):\n",
    "            df = cargar_noticias_ElMundo(\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        elif(filtro=='Todos'):\n",
    "            df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "            \n",
    "        else:\n",
    "            df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "            dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "            rankingLimitado2 = []\n",
    "            textoRanking2 = []\n",
    "            for i in range(int(topN)):\n",
    "                i+=1\n",
    "                # REVISAR\n",
    "                rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "                textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "            window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "        #dataRanking2 = compararNoticiasActualzado(textoConsulta,df)\n",
    "        #rankingLimitado2 = []\n",
    "        #textoRanking2 = []\n",
    "        #for i in range(int(topN)):\n",
    "         #   i+=1\n",
    "            #REVISAR\n",
    "          #  rankingLimitado2.append(str(int(dataRanking2.porcentage[i]*100))+\" % de coincidencia\")\n",
    "           # textoRanking2.append(dataRanking2.data[i])   \n",
    "\n",
    "        #window.find_element('listbox_ranking_noticias').Update(values=rankingLimitado2)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PySimpleGUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-ability",
   "metadata": {},
   "source": [
    "# PRUEBA.) Tratamiento de las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes de un documento con el resto (el  resto estan en un dataframe)\n",
    "def compararNoticias(texto, df):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similitudes = []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            X = vectorizer.fit_transform([texto,df.data[i]])\n",
    "            similarity_matrix = cosine_similarity(X,X)\n",
    "            similitudes.append(float(similarity_matrix[1][0]))\n",
    "            i+=1\n",
    "        except:\n",
    "            i+=1\n",
    "    similitudes.sort(reverse=True) # Los ordemanos de mayor a menor \n",
    "    similitudes.remove(similitudes[0]) # Eliminamos el primer valor porque nos da la similitud con el propio elemento, por loo que siempre va a ser 1 (no nos sirve de nada)\n",
    "    return similitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-budget",
   "metadata": {},
   "source": [
    "Este método guardamos en un dataframe \"df\" los diferentes ficheros de cada tematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia,ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia,ruta_PaisCiencia,ruta_PaisSanidad,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSanidad = os.listdir(ruta_PaisSanidad)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "\n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSanidad:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSanidad+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar un documento (obtenemos los vectores de los documentos que queramos)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer ()\n",
    "\n",
    "X = vectorizer.fit_transform([df.data[1],df.data[130]])\n",
    "vectorizer.get_feature_names()\n",
    "vectorDoc = X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF. Similitud entre dos documentos\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer ()\n",
    "X = vectorizer.fit_transform([df.data[1],df.data[250]])\n",
    "similarity_matrix = cosine_similarity(X,X)\n",
    "similarity_matrix[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes de un documento con el resto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similitudes = []\n",
    "for i in range(len(df)):\n",
    "    vectorizer = TfidfVectorizer ()\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer ()\n",
    "        X = vectorizer.fit_transform([df.data[250],df.data[i]])\n",
    "        similarity_matrix = cosine_similarity(X,X)\n",
    "        similitudes.append(float(similarity_matrix[1][0]))\n",
    "        i+=1\n",
    "    except:\n",
    "        i+=1\n",
    "similitudes.sort(reverse=True) # Los ordemanos de mayor a menor \n",
    "similitudes.remove(similitudes[0]) # Eliminamos el primer valor porque nos da la similitud con el propio elemento, por loo que siempre va a ser 1 (no nos sirve de nada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar un documento y calcular su similitud con todos los documentos\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i in range(len(df)):\n",
    "    vectorizer = TfidfVectorizer ()\n",
    "    try:\n",
    "        X = vectorizer.fit_transform([df.data[1],df.data[i]])\n",
    "        vectorDoc = X.toarray()\n",
    "        print(vectorDoc)\n",
    "        i+=1\n",
    "    except:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una lista con los símbolos de puntuación y las stopwords para que sean retiradas del texto\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lista_parada = open(os.getcwd()+\"/stopwords.txt\",\"r\").read().split()\n",
    "non_words = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para el preprocesado del texto\n",
    "def remove_stop_words(dirty_text):\n",
    "    cleaned_text = ''\n",
    "    for word in dirty_text.split():\n",
    "        if word in language_stopwords or word in non_words:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_text += word + ' '\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation(dirty_string):\n",
    "    for word in non_words:\n",
    "        dirty_string = dirty_string.replace(word, '')\n",
    "    return dirty_string\n",
    "\n",
    "def process_file(file_name):\n",
    "    file_content = open(file_name, \"r\").read()\n",
    "    # All to lower case\n",
    "    file_content = file_content.lower()\n",
    "    # Remove punctuation and spanish stopwords\n",
    "    file_content = remove_punctuation(file_content)\n",
    "    file_content = remove_stop_words(file_content)\n",
    "    return file_content\n",
    "\n",
    "def procesado(ruta):\n",
    "    for file in (os.listdir(ruta)):\n",
    "        process_file(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesado(\"20Minutos/Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el preprocesado del texto\n",
    "import os\n",
    "\n",
    "os.listdir(\"20Minutos/Salud\")\n",
    "os.listdir(\"20Minutos/Ciencia\")\n",
    "os.listdir(\"20Minutos/Tecnologia\")\n",
    "os.listdir(\"ElMundo/Ciencia\")\n",
    "os.listdir(\"ElMundo/Salud\")\n",
    "os.listdir(\"ElMundo/Tecnologia\")\n",
    "os.listdir(\"ElPais/Ciencia\")\n",
    "os.listdir(\"ElPais/Sanidad\")\n",
    "os.listdir(\"ElPais/Tecnologia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-african",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
