{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "musical-container",
   "metadata": {},
   "source": [
    "# Con que páginas vamos a trabajar\n",
    "• El Mundo (https://elmundo.es) \n",
    "- Salud: https://www.elmundo.es/ciencia-y-salud/salud.html \n",
    "- Tecnología: https://www.elmundo.es/tecnologia.html \n",
    "- Ciencia: https://www.elmundo.es/ciencia-y-salud/ciencia.html \n",
    " \n",
    "• El País (https://elpais.com/) \n",
    "- Sanidad: https://elpais.com/noticias/sanidad/ \n",
    "- Tecnología: https://elpais.com/tecnologia/ \n",
    "- Ciencia: https://elpais.com/ciencia/ \n",
    "\n",
    "• 20 Minutos (https://www.20minutos.es/) \n",
    "- Salud: https://www.20minutos.es/salud/ \n",
    "- Tecnología: https://www.20minutos.es/tecnologia/ \n",
    "- Ciencia: https://www.20minutos.es/ciencia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-links",
   "metadata": {},
   "source": [
    "# Importamos las librerias que vamos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "environmental-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-convert",
   "metadata": {},
   "source": [
    "# 1.) Comenzamos accediendo a las URLs y sacando los links de las noticias de cada campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElMundoSalud = requests.get(\"https://www.elmundo.es/ciencia-y-salud/salud.html\")\n",
    "ElMundoTecnologia = requests.get(\"https://www.elmundo.es/tecnologia.html\")\n",
    "ElMundoCiencia = requests.get(\"https://www.elmundo.es/ciencia-y-salud/ciencia.html\")\n",
    "\n",
    "ElPaisSanidad = requests.get(\"https://elpais.com/noticias/sanidad/\")\n",
    "ElPaisTecnologia = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "ElPaisCiencia = requests.get(\"https://elpais.com/ciencia/\")\n",
    "\n",
    "MinutosSalud = requests.get(\"https://www.20minutos.es/salud/\")\n",
    "MinutosTecnologia = requests.get(\"https://www.20minutos.es/tecnologia/\")\n",
    "MinutosCiencia = requests.get(\"https://www.20minutos.es/ciencia/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a ver si ha accedido correctamente a todas las paginas (200)\n",
    "print(ElMundoSalud.status_code)\n",
    "print(ElMundoTecnologia.status_code)\n",
    "print(ElMundoCiencia.status_code)\n",
    "\n",
    "print(ElPaisSanidad.status_code)\n",
    "print(ElPaisTecnologia.status_code)\n",
    "print(ElPaisCiencia.status_code)\n",
    "\n",
    "print(MinutosSalud.status_code)\n",
    "print(MinutosTecnologia.status_code)\n",
    "print(MinutosCiencia.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descargamos el contenido de las páginas\n",
    "ContenidoElMundoSalud = BeautifulSoup (ElMundoSalud.content, 'html.parser')\n",
    "ContenidoElMundoTecnologia = BeautifulSoup (ElMundoTecnologia.content, 'html.parser')\n",
    "ContenidoElMundoCiencia = BeautifulSoup (ElMundoCiencia.content, 'html.parser')\n",
    "\n",
    "ContenidoElPaisSanidad = BeautifulSoup (ElPaisSanidad.content, 'html.parser')\n",
    "ContenidoElPaisTecnologia = BeautifulSoup (ElPaisTecnologia.content, 'html.parser')\n",
    "ContenidoElPaisCiencia = BeautifulSoup (ElPaisCiencia.content, 'html.parser')\n",
    "\n",
    "ContenidoMinutosSalud = BeautifulSoup (MinutosSalud.content, 'html.parser')\n",
    "ContenidoMinutosTecnologia = BeautifulSoup (MinutosTecnologia.content, 'html.parser')\n",
    "ContenidoMinutosCiencia = BeautifulSoup (MinutosCiencia.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-score",
   "metadata": {},
   "source": [
    "## Noticias El Mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenLinksMundo(contenido, array):\n",
    "    a = contenido.find_all('a', class_=\"ue-c-cover-content__link\")\n",
    "    for link in a:\n",
    "        array.append(link[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-agency",
   "metadata": {},
   "source": [
    "#### Salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksSalud = []\n",
    "obtenLinksMundo(ContenidoElMundoSalud, linksSalud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-carry",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksTecnologia = []\n",
    "obtenLinksMundo(ContenidoElMundoTecnologia, linksTecnologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-allen",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksCiencia = []\n",
    "obtenLinksMundo(ContenidoElMundoCiencia, linksCiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-participation",
   "metadata": {},
   "source": [
    "## Noticias El País"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenLinksPais(contenido, array):\n",
    "    h2 = contenido.find_all('h2', class_=\"c_t\")\n",
    "    for a in h2:\n",
    "        for link in a:\n",
    "            array.append(\"https://elpais.com/\"+link[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-consortium",
   "metadata": {},
   "source": [
    "#### Sanidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksSanidadElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisSanidad, linksSanidadElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-kentucky",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksTecnologiaElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisTecnologia, linksTecnologiaElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-realtor",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksCienciaElPáis = []\n",
    "obtenLinksPais(ContenidoElPaisCiencia, linksCienciaElPáis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-farming",
   "metadata": {},
   "source": [
    "### Noticias 20 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo para recuperar todos los links de \"20 Minutos\"\n",
    "\n",
    "def obtenLinksMinutos(Contenido, array):\n",
    "    div = Contenido.find_all('div', class_='media-content')\n",
    "    for header in div:\n",
    "        for a in header.findAll(\"a\"):\n",
    "            array.append(a[\"href\"])\n",
    "    return len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-degree",
   "metadata": {},
   "source": [
    "#### Salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksMinutosSalud = []\n",
    "obtenLinksMinutos(ContenidoMinutosSalud, linksMinutosSalud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-tobago",
   "metadata": {},
   "source": [
    "#### Tecnologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksMinutosTecnologia = []\n",
    "obtenLinksMinutos(ContenidoMinutosTecnologia, linksMinutosTecnologia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-korea",
   "metadata": {},
   "source": [
    "#### Ciencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksMinutosCiencia = []\n",
    "obtenLinksMinutos(ContenidoMinutosCiencia, linksMinutosCiencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-kingston",
   "metadata": {},
   "source": [
    "# 2.) Vamos a sacar el contenido que queremos de cada pagina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-produce",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"El Mundo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosMundo(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"ElMundo/\"+area+\"/\"+str(area)+\".\"+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"ue-c-article__headline\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('p', class_=\"ue-c-article__standfirst\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            archivo.write(str(titulo)+\"\\n\\n\"+str(subtitulo)+\"\\n\")\n",
    "\n",
    "            #Este div contiene todos los \"p\" con el contenido de la noticia  \n",
    "            div = soup.findAll('div', class_=\"ue-l-article__body ue-c-article__body\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    archivo.write(\"\\n\"+each.get_text())\n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksSalud, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksTecnologia, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMundo(linksCiencia, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-imagination",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"El País\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosPais(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"ElPais/\"+area+\"/\"+str(area)+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"a_t\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                subtitulo = soup.find('h2', class_=\"a_st\").get_text()\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "\n",
    "            archivo.write(str(titulo)+\"\\n\\n\"+str(subtitulo)+\"\\n\")\n",
    "            \n",
    "            # Contenido de la noticia\n",
    "            div = soup.findAll('div', class_=\"a_c clearfix\")\n",
    "            for p in div:\n",
    "                for each in p.findAll(\"p\"): \n",
    "                    try:\n",
    "                        archivo.write(\"\\n\"+each.get_text())\n",
    "                    except:\n",
    "                        archivo.write(\" \")\n",
    "\n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksSanidadElPáis, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksCienciaElPáis, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosPais(linksTecnologiaElPáis, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-minimum",
   "metadata": {},
   "source": [
    "### Contenido de las noticias de \"20 Minutos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todo el contenido de la noticia\n",
    "def contenidoArticulolosMinutos(array, area): # array con las urls, area (Salud, Tecnología, Ciencia)\n",
    "    for i in range(20):\n",
    "        req = requests.get(array[i])\n",
    "        if((req.status_code) == 200):\n",
    "            soup = BeautifulSoup (req.content, 'html.parser')\n",
    "            archivo = open(\"20Minutos/\"+area+\"/\"+str(area)+\".\"+datetime.today().strftime('%Y-%m-%d')+\".\"+str(i).zfill(3)+\".txt\", \"w\")\n",
    "            # Con un try catch hacemos que si el articulo no tiene un titulo lo deje en blanco\n",
    "            try:\n",
    "                titulo = soup.find('h1', class_=\"article-title\").string\n",
    "            except:\n",
    "                titulo = \"\"\n",
    "                \n",
    "            archivo.write(str(titulo)+\"\\n\\n\")\n",
    "                \n",
    "            # Con un try catch hacemos que si el articulo no tiene un subtitulo lo deje en blanco\n",
    "            try:\n",
    "                div = soup.findAll('div', class_=\"article-intro\")\n",
    "                for li in div:\n",
    "                    for each in li.findAll(\"li\"): \n",
    "                        archivo.write(str(each.get_text()))\n",
    "            except:\n",
    "                subtitulo = \"\"\n",
    "                \n",
    "            archivo.write(\"\\n\\n\")\n",
    "                \n",
    "            # Contenido de la noticia\n",
    "            p = soup.findAll('p', class_=\"paragraph\")\n",
    "            for cont in p: \n",
    "                try:\n",
    "                    archivo.write(cont.get_text())\n",
    "                except:\n",
    "                    archivo.write(\" \")\n",
    "            \n",
    "            archivo.close()\n",
    "        else:\n",
    "            print(\"Error al acceder a las paginas.\")\n",
    "            break\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosSalud, \"Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosCiencia, \"Ciencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "contenidoArticulolosMinutos(linksMinutosTecnologia, \"Tecnologia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-request",
   "metadata": {},
   "source": [
    "# 3.) Tratamiento de las noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-ordinary",
   "metadata": {},
   "source": [
    "Este método guardamos en un dataframe \"df\" los diferentes ficheros de cada tematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greatest-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia,ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia,ruta_PaisCiencia,ruta_PaisSanidad,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSanidad = os.listdir(ruta_PaisSanidad)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "\n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSanidad:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSanidad+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-ultimate",
   "metadata": {},
   "source": [
    "Este metodo compara una noticia con el resto dentro del df y devuelve un dataframe con cada texto y la similitud que tiene con el texto introducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "specified-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitivo\n",
    "def compararNoticiasActualzado(textoin, df):\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    newdf = pd.DataFrame(columns = ['porcentage','data'])\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        X = vectorizer.fit_transform([textoin,df.data[i]])\n",
    "        similarity_matrix = cosine_similarity(X,X)\n",
    "        porcentage = float(similarity_matrix[1][0])\n",
    "        #print(porcentaje)\n",
    "        texto = df.data[i]\n",
    "        new_row = {'porcentage':porcentage,'data': texto}\n",
    "        newdf = newdf.append(new_row, ignore_index=True)\n",
    "        i+=1\n",
    "    \n",
    "    newdf = newdf.sort_values('porcentage',ascending=False)\n",
    "    newdf = newdf.reset_index()\n",
    "    newdf = newdf.drop(['index'], axis=1)\n",
    "    newdf = newdf.drop([0],axis=0)\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-mountain",
   "metadata": {},
   "source": [
    "# 4.) Ventana (GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "small-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CON LA VENTANA\n",
    "\n",
    "import os\n",
    "import PySimpleGUI as sg\n",
    "sg.theme('BlueMono')   # Add a little color to your windows\n",
    "# All the stuff inside your window. This is the PSG magic code compactor...\n",
    "\n",
    "listaNumero = [\"1\", \"2\", \"3\", \"4\", \"5\"] #numero de noticias que se quieren en el ranking\n",
    "list1 = [\"Origen\", \"Todos\"] #Filtrar las noticias por el medio de origen o seleccionar todos\n",
    "params = 10\n",
    "medios = [\"20 minutos\", \"El Pais\", \"El Mundo\"]\n",
    "categorias = [\"Ciencia\", \"Salud\", \"Tecnologia\"]\n",
    "noticias = [\"noticia 1\", \"noticia 2\", \"noticia 3\", \"noticia 4\", \"noticia 5\", \"noticia 6\", \"noticia 7\"]\n",
    "noticiasPorcentajes = [\"noticia 1   86%\", \"noticia 2  78%\", \"noticia 3  70%\", \"noticia 4  68%\", \"noticia 5  62&\", \"noticia 6  59%\", \"noticia 7  58%\"]\n",
    "\n",
    "\n",
    "busqueda = [[sg.Text('Consulta:'), sg.InputText(size=80)],\n",
    "            [sg.Text('TOP-N:'),sg.Combo(listaNumero, size = params),sg.Text('             '), sg.Text('Filtrar:'), sg.Combo(list1, size = params),sg.Text('                              '), sg.Button('Buscar', size=10)],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('Ranking:'), sg.Text('                                                             '), sg.Text('Texto de la noticia:')],\n",
    "            [sg.Listbox(noticiasPorcentajes, size=(30, 10), key='-LISTBOX-'),sg.Text('             ') ,sg.Multiline(size = (50, 10), key = 'textbox')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "comparar = [[sg.Text('Medio:'), sg.Text('                                  '), sg.Text('Categoría:'), sg.Text('                                                             '), sg.Text('Noticias:')],\n",
    "            [sg.Listbox(medios, size=(20, 4), key='listbox_medios'),sg.Text('     '), sg.Listbox(categorias, size=(20, 4), key='listbox_categorias'),sg.Text('     '), sg.Button(\"Ver\\nnoticias\", size=(10,2), key='btn_verNoticias'),sg.Text('     '), sg.Listbox(values=[], size=(30, 4), key='listbox_noticias')],\n",
    "            [sg.Button('Preview', key='btn_preview')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview')],\n",
    "            [sg.Text('                                      '), sg.Text('TOP-N:'), sg.Combo(listaNumero, size = params, key='combo_topN'),sg.Text('             '), sg.Button('Buscar', size=10, key='btn_buscar')],\n",
    "            [sg.Text('')],\n",
    "            [sg.Text('                        '), sg.Text('Filtrar:'), sg.Combo(medios, size = params),sg.Text('Ranking:'), sg.Listbox(values=\"\", size=(30, 5), key='noticias_porcentajes'), sg.Button('Preview', key='btn_preview_reslutado')],\n",
    "            [sg.Text('Texto de la noticia:')],\n",
    "            [sg.Multiline(size = (107, 7), key = 'textbox_preview_reslutado')],\n",
    "            [sg.Button('Salir', size=10)]\n",
    "         ]\n",
    "tabgrp = [\n",
    "    [sg.TabGroup([[ sg.Tab('Comparador de noticias', comparar),\n",
    "                    sg.Tab('Buscador de noticias', busqueda)\n",
    "                ]], tab_location='top')]]\n",
    "\n",
    "# Create the Window\n",
    "window = sg.Window('Buscador y comparador de noticias', tabgrp)\n",
    "# Event Loop to process \"events\"\n",
    "while True:             \n",
    "    event, values = window.read()\n",
    "    if event in (sg.WIN_CLOSED, 'Salir'):\n",
    "        break\n",
    "    if event == \"btn_verNoticias\":\n",
    "        eleccion = values['listbox_medios']+values['listbox_categorias']\n",
    "        print(eleccion)\n",
    "        if(eleccion==['20 minutos', 'Ciencia']):\n",
    "            noticias = os.listdir(\"20Minutos/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['20 minutos', 'Salud']):\n",
    "            noticias = os.listdir(\"20Minutos/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['20 minutos', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"20Minutos/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "        elif(eleccion==['El Pais', 'Ciencia']):\n",
    "            noticias = os.listdir(\"ElPais/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Pais', 'Salud']):\n",
    "            noticias = os.listdir(\"ElPais/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Pais', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"ElPais/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "        elif(eleccion==['El Mundo', 'Ciencia']):\n",
    "            noticias = os.listdir(\"ElMundo/Ciencia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Mundo', 'Salud']):\n",
    "            noticias = os.listdir(\"ElMundo/Salud\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "        elif(eleccion==['El Mundo', 'Tecnologia']):\n",
    "            noticias = os.listdir(\"ElMundo/Tecnologia\")\n",
    "            window.FindElement('listbox_noticias').Update(values=noticias)\n",
    "            print(values['listbox_noticias'])\n",
    "            \n",
    "    if event==\"btn_preview\":\n",
    "        noticia = values['listbox_noticias']\n",
    "        f = open(eleccion[0].replace(\" \", \"\")+\"/\"+eleccion[1]+\"/\"+noticia[0], \"r\")\n",
    "        texto = f.read()\n",
    "        window['textbox_preview'].update(texto)\n",
    "        \n",
    "    if event==\"btn_buscar\":\n",
    "        eleccion = values['listbox_medios']+values['listbox_categorias']\n",
    "        noticia = values['listbox_noticias']\n",
    "        topN=values['combo_topN']\n",
    "        dirNoticia = open(eleccion[0].replace(\" \", \"\")+\"/\"+eleccion[1]+\"/\"+noticia[0], \"r\")\n",
    "        textoNoticia = dirNoticia.read()\n",
    "        df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "        dataRanking = compararNoticiasActualzado(textoNoticia,df)\n",
    "        rankingLimitado = []\n",
    "        textoRanking = []\n",
    "        for i in range(int(topN)):\n",
    "            i+=1\n",
    "            # REVISAR\n",
    "            rankingLimitado.append(str(int(dataRanking.porcentage[i]*100))+\" % de coincidencia\")\n",
    "            textoRanking.append(dataRanking.data[i])   \n",
    "\n",
    "        window.find_element('noticias_porcentajes').Update(values=rankingLimitado)\n",
    "        \n",
    "    if event==\"btn_preview_reslutado\":\n",
    "        textoResultado = \"\"        \n",
    "        for i in range(len(rankingLimitado)):\n",
    "            if str(values['noticias_porcentajes'][0])==str(rankingLimitado[i]):   \n",
    "                textoResultado = textoRanking[i]\n",
    "            i+=1\n",
    "        window['textbox_preview_reslutado'].update(textoResultado)\n",
    "\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-control",
   "metadata": {},
   "source": [
    "# PRUEBA.) Tratamiento de las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes de un documento con el resto (el  resto estan en un dataframe)\n",
    "def compararNoticias(texto, df):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similitudes = []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            X = vectorizer.fit_transform([texto,df.data[i]])\n",
    "            similarity_matrix = cosine_similarity(X,X)\n",
    "            similitudes.append(float(similarity_matrix[1][0]))\n",
    "            i+=1\n",
    "        except:\n",
    "            i+=1\n",
    "    similitudes.sort(reverse=True) # Los ordemanos de mayor a menor \n",
    "    similitudes.remove(similitudes[0]) # Eliminamos el primer valor porque nos da la similitud con el propio elemento, por loo que siempre va a ser 1 (no nos sirve de nada)\n",
    "    return similitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-weapon",
   "metadata": {},
   "source": [
    "Este método guardamos en un dataframe \"df\" los diferentes ficheros de cada tematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "toxic-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_noticias(ruta_20MinSalud,ruta_20MinCiencia,ruta_20MinTecnologia,ruta_MundoCiencia,ruta_MundoSalud,ruta_MundoTecnologia,ruta_PaisCiencia,ruta_PaisSanidad,ruta_PaisTecnologia):\n",
    "    import os\n",
    "    #extraemos todos los ficheros que hay en acda carpeta\n",
    "    files_20MinSalud = os.listdir(ruta_20MinSalud)\n",
    "    files_20MinCiencia = os.listdir(ruta_20MinCiencia)\n",
    "    files_20MinTecnologia = os.listdir(ruta_20MinTecnologia)\n",
    "    files_MundoCiencia = os.listdir(ruta_MundoCiencia)\n",
    "    files_MundoSalud = os.listdir(ruta_MundoSalud)\n",
    "    files_MundoTecnologia = os.listdir(ruta_MundoTecnologia)\n",
    "    files_PaisCiencia = os.listdir(ruta_PaisCiencia)\n",
    "    files_PaisSanidad = os.listdir(ruta_PaisSanidad)\n",
    "    files_PaisTecnologia = os.listdir(ruta_PaisTecnologia)\n",
    "    \n",
    "    #importamos a un df las noticias\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns = ['file','data'])\n",
    "    \n",
    "    #Importamos ficheros de 20MinSalud\n",
    "    for file in files_20MinSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "\n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinCiencia\n",
    "    for file in files_20MinCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de 20MinTecnologia\n",
    "    for file in files_20MinTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_20MinTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoCiencia\n",
    "    for file in files_MundoCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoSalud\n",
    "    for file in files_MundoSalud:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoSalud+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de MundoTecnologia\n",
    "    for file in files_MundoTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_MundoTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisCiencia\n",
    "    for file in files_PaisCiencia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisCiencia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisSanidad\n",
    "    for file in files_PaisSanidad:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisSanidad+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "            \n",
    "    #Importamos ficheros de PaisTecnologia\n",
    "    for file in files_PaisTecnologia:\n",
    "        try:\n",
    "            fichero_abierto = open(ruta_PaisTecnologia+\"/\"+file, \"r\")\n",
    "            texto=fichero_abierto.read()\n",
    "        \n",
    "            #La lista de parada se ejecuta despues al vectorizar\n",
    "\n",
    "            #Añadimos el nombre del fichero al df y su tipo\n",
    "            new_row = {'file':file,'data': texto}\n",
    "            df=df.append(new_row, ignore_index=True)\n",
    "        except:\n",
    "            fichero_abierto.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "damaged-biotechnology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salud.2022-01-06.000.txt</td>\n",
       "      <td>Así es vivir con la extraña enfermedad que tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Salud.2022-01-06.001.txt</td>\n",
       "      <td>¿Qué es el síndrome de la vida ocupada? Motivo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salud.2022-01-06.002.txt</td>\n",
       "      <td>Tres infusiones para ayudar a bajar los trigli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Salud.2022-01-06.003.txt</td>\n",
       "      <td>¿Qué es el bocio? Estas son sus causas, síntom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salud.2022-01-06.004.txt</td>\n",
       "      <td>Este es el peso que tienes que levantar para g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Tecnologia2022-01-06.015.txt</td>\n",
       "      <td>\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Tecnologia2022-01-06.016.txt</td>\n",
       "      <td>Recetas para la reindustrialización: innovació...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Tecnologia2022-01-06.017.txt</td>\n",
       "      <td>El futuro de la industria en Series Retina\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Tecnologia2022-01-06.018.txt</td>\n",
       "      <td>Ideas para descarbonizar la economía\\n\\nCooper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Tecnologia2022-01-06.019.txt</td>\n",
       "      <td>La descarbonización de la economía, en los eve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file  \\\n",
       "0        Salud.2022-01-06.000.txt   \n",
       "1        Salud.2022-01-06.001.txt   \n",
       "2        Salud.2022-01-06.002.txt   \n",
       "3        Salud.2022-01-06.003.txt   \n",
       "4        Salud.2022-01-06.004.txt   \n",
       "..                            ...   \n",
       "175  Tecnologia2022-01-06.015.txt   \n",
       "176  Tecnologia2022-01-06.016.txt   \n",
       "177  Tecnologia2022-01-06.017.txt   \n",
       "178  Tecnologia2022-01-06.018.txt   \n",
       "179  Tecnologia2022-01-06.019.txt   \n",
       "\n",
       "                                                  data  \n",
       "0    Así es vivir con la extraña enfermedad que tra...  \n",
       "1    ¿Qué es el síndrome de la vida ocupada? Motivo...  \n",
       "2    Tres infusiones para ayudar a bajar los trigli...  \n",
       "3    ¿Qué es el bocio? Estas son sus causas, síntom...  \n",
       "4    Este es el peso que tienes que levantar para g...  \n",
       "..                                                 ...  \n",
       "175                                             \\n\\n\\n  \n",
       "176  Recetas para la reindustrialización: innovació...  \n",
       "177  El futuro de la industria en Series Retina\\n\\n...  \n",
       "178  Ideas para descarbonizar la economía\\n\\nCooper...  \n",
       "179  La descarbonización de la economía, en los eve...  \n",
       "\n",
       "[180 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cargar_noticias(\"20Minutos/Salud\",\"20Minutos/Ciencia\",\"20Minutos/Tecnologia\",\"ElMundo/Ciencia\",\"ElMundo/Salud\",\"ElMundo/Tecnologia\",\"ElPais/Ciencia\",\"ElPais/Salud\",\"ElPais/Tecnologia\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aging-coach",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar un documento (obtenemos los vectores de los documentos que queramos)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer ()\n",
    "\n",
    "X = vectorizer.fit_transform([df.data[1],df.data[130]])\n",
    "vectorizer.get_feature_names()\n",
    "vectorDoc = X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "respiratory-edition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6286208152097036"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF. Similitud entre dos documentos usando sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer ()\n",
    "X = vectorizer.fit_transform([df.data[1],df.data[50]])\n",
    "similarity_matrix = cosine_similarity(X,X)\n",
    "similarity_matrix[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "distant-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes de un documento con el resto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similitudes = []\n",
    "for i in range(len(df)):\n",
    "    vectorizer = TfidfVectorizer ()\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer ()\n",
    "        X = vectorizer.fit_transform([df.data[150],df.data[i]])\n",
    "        similarity_matrix = cosine_similarity(X,X)\n",
    "        similitudes.append(float(similarity_matrix[1][0]))\n",
    "        i+=1\n",
    "    except:\n",
    "        i+=1\n",
    "similitudes.sort(reverse=True) # Los ordemanos de mayor a menor \n",
    "similitudes.remove(similitudes[0]) # Eliminamos el primer valor porque nos da la similitud con el propio elemento, por loo que siempre va a ser 1 (no nos sirve de nada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "conditional-dietary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8648749630513428, 0.8510984683396574, 0.8362819495223599, 0.8362819495223599, 0.8362819495223599, 0.835929925175501, 0.832598116020617, 0.8255012289038266, 0.8254744820095625, 0.8204580673025599, 0.8201175798158398, 0.8201175798158398, 0.8189838199773779, 0.8154810533663633, 0.8106118419971002, 0.8087953558949276, 0.8055138540396345, 0.8039677758017804, 0.8037082909444424, 0.8021129897048411, 0.8012572043732922, 0.8011723059139656, 0.7994007575668155, 0.7987776409224245, 0.7987776409224245, 0.7955269828078396, 0.7947033402806173, 0.7947033402806173, 0.7939314152524096, 0.793105637916124, 0.7926692871571321, 0.7911579158867201, 0.7910745945588437, 0.790523466677055, 0.7868077606457117, 0.785950540016007, 0.7848321464698629, 0.7842539710241322, 0.7823116545747667, 0.7817680871755889, 0.7817680871755889, 0.7813131168900572, 0.7805164692086594, 0.7799842948722605, 0.7796856763016258, 0.7760708323081648, 0.7754919365452759, 0.7722750742083521, 0.7722750742083521, 0.7710952810534482, 0.7692474244430239, 0.769132453469581, 0.7678305154031875, 0.7670737622682758, 0.7670737622682758, 0.7636632907090883, 0.7636632907090883, 0.7603780019966787, 0.7542201620040768, 0.7530081498466915, 0.7522930502128374, 0.7513465113200812, 0.749392769202515, 0.7482957563142318, 0.7475172168431751, 0.7453887381488963, 0.7444795872109605, 0.7422599273353755, 0.7416440005409806, 0.7410717572246079, 0.7397286625907592, 0.7387734796621647, 0.7378377122603167, 0.7364959570709844, 0.7364959570709844, 0.7364959570709844, 0.7360863769320065, 0.7359642629947964, 0.7358456844046047, 0.7337362290991712, 0.7327230935925836, 0.7318147363987229, 0.7310992123417777, 0.7310171495797736, 0.7288175157798368, 0.7281101167673154, 0.7249717537498755, 0.7247877948149557, 0.7246018811634651, 0.7217361129326547, 0.7187509724803446, 0.7159440248217872, 0.714360027227637, 0.7142474093058967, 0.7140331396306515, 0.7116640547496162, 0.7081656810909329, 0.7066431719664071, 0.7056391308450629, 0.7039678651559005, 0.7029184079375336, 0.6996834858748942, 0.6995679820653354, 0.6995679820653354, 0.6995679820653354, 0.6990251728472041, 0.6965662807187725, 0.6947982759015352, 0.6937555360178825, 0.6905840592635554, 0.6865734629278852, 0.6865734629278852, 0.682189027841014, 0.6800909641014179, 0.6800909641014179, 0.6784178903134607, 0.6781283313009411, 0.6765966963931761, 0.6736490383882208, 0.6726753848353151, 0.6717939772361721, 0.6713615256360068, 0.6684403940408472, 0.6667041645137487, 0.6660460083489232, 0.6626543271716446, 0.6626543271716446, 0.6621437226029012, 0.6619940916681208, 0.6619940916681208, 0.6613389881212202, 0.6593888068645235, 0.6590327628166082, 0.6590327628166082, 0.6573110977003841, 0.6569603407608194, 0.6563661968963176, 0.6560357867268793, 0.6549412315637879, 0.6549412315637879, 0.6543798186250607, 0.6509794772067198, 0.6503449987088677, 0.6465604201537158, 0.6444321764180563, 0.6373999057694245, 0.6357835352077154, 0.6349198632494765, 0.6326028269678527, 0.63044687023664, 0.63044687023664, 0.6250527527980799, 0.6250527527980799, 0.6083201642060722, 0.6071587333284986, 0.6011247656741379, 0.6011247656741379, 0.5993873778133665, 0.5965256111446787, 0.5948875306748124, 0.5902781136707881, 0.5847807319874836, 0.5798181142000878, 0.5676113724513934, 0.5637885546778991, 0.5637885546778991, 0.5636688963766353, 0.5495679913732996, 0.5442652331929445, 0.5442652331929445, 0.5438909212161278, 0.5398469060671479, 0.5390376750287008, 0.5057864358514851, 0.4057910720903123, 0.3389013246242068, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(similitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-stereo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar un documento y calcular su similitud con todos los documentos\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i in range(len(df)):\n",
    "    vectorizer = TfidfVectorizer ()\n",
    "    try:\n",
    "        X = vectorizer.fit_transform([df.data[1],df.data[i]])\n",
    "        vectorDoc = X.toarray()\n",
    "        print(vectorDoc)\n",
    "        i+=1\n",
    "    except:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una lista con los símbolos de puntuación y las stopwords para que sean retiradas del texto\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "lista_parada = open(os.getcwd()+\"/stopwords.txt\",\"r\").read().split()\n",
    "non_words = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para el preprocesado del texto\n",
    "def remove_stop_words(dirty_text):\n",
    "    cleaned_text = ''\n",
    "    for word in dirty_text.split():\n",
    "        if word in language_stopwords or word in non_words:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_text += word + ' '\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation(dirty_string):\n",
    "    for word in non_words:\n",
    "        dirty_string = dirty_string.replace(word, '')\n",
    "    return dirty_string\n",
    "\n",
    "def process_file(file_name):\n",
    "    file_content = open(file_name, \"r\").read()\n",
    "    # All to lower case\n",
    "    file_content = file_content.lower()\n",
    "    # Remove punctuation and spanish stopwords\n",
    "    file_content = remove_punctuation(file_content)\n",
    "    file_content = remove_stop_words(file_content)\n",
    "    return file_content\n",
    "\n",
    "def procesado(ruta):\n",
    "    for file in (os.listdir(ruta)):\n",
    "        process_file(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesado(\"20Minutos/Salud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el preprocesado del texto\n",
    "import os\n",
    "\n",
    "os.listdir(\"20Minutos/Salud\")\n",
    "os.listdir(\"20Minutos/Ciencia\")\n",
    "os.listdir(\"20Minutos/Tecnologia\")\n",
    "os.listdir(\"ElMundo/Ciencia\")\n",
    "os.listdir(\"ElMundo/Salud\")\n",
    "os.listdir(\"ElMundo/Tecnologia\")\n",
    "os.listdir(\"ElPais/Ciencia\")\n",
    "os.listdir(\"ElPais/Sanidad\")\n",
    "os.listdir(\"ElPais/Tecnologia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-adult",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
